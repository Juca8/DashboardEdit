# -*- coding: utf-8 -*-
"""FinalModel_Challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HZCD868Ez4GgyLEPHPn1V__ThNw6oj9R
"""

"""# **Stratify**
Main objective	Understand consumption behavior of allies

What we want to understand?
- Basic demand (e.,g, type of products, type, recypient)
- Demand Behaviour (e.g., frequency, time between loans, loans simultaneously)
- Communication strategy (e.g., when to reach out, potential risk flags)
- Split between cliente final and mercancia
"""

import warnings

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from math import pi
import unicodedata

from sklearn.utils import resample

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score
from sklearn.impute import SimpleImputer

from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

import joblib
import streamlit as st

import os

file_path = "Aliadas_team2.csv"

if os.path.exists(file_path):
    df_aliadas_team2 = pd.read_csv(file_path, encoding="latin1")
else:
    st.error(f"File not found: {file_path}")
    
df_aliadas_team2

df_aliadas_team2.columns, df_aliadas_team2.shape

df_aliadas_team2.dtypes

df_prestamos_team2 = pd.read_csv("/content/Prestamos_team2.csv",encoding = "latin1")

df_prestamos_team2.head()

df_prestamos_team2.columns, df_prestamos_team2.shape

df_prestamos_team2.isna().sum(), df_prestamos_team2.shape

df_merged = pd.merge(df_aliadas_team2, df_prestamos_team2, left_on="ID Aliada", right_on="Ally ID", how="left")
df_merged.head()

df_merged.columns, df_merged.shape

df_merged.isna().sum(),df_merged.shape

numeric_cols = df_merged.select_dtypes(include=[np.number]).columns
cols_with_nan = [col for col in numeric_cols if df_merged[col].isna().any()]

for col in cols_with_nan:
    na_indices = df_merged[df_merged[col].isna()].index
    valores_existentes = df_merged[col].dropna()

    mean = valores_existentes.mean()
    std = valores_existentes.std()

    for idx in na_indices:
        simulaciones = np.random.normal(loc=mean, scale=std, size=1000)
        simulaciones = np.clip(simulaciones, 0, np.inf)
        valor_imputado = np.mean(simulaciones)
        df_merged.at[idx, col] = valor_imputado

df_merged['Total_Llamadas'] = (
    df_merged['Llamadas_Colgadas'].fillna(0) +
    df_merged['Llamada_Contestada'].fillna(0)
)

df_merged.isna().sum(),df_merged.shape

# Step 0: Numeric conversion (ensure numeric columns are properly formatted)
numeric_columns = [
    'cuotas_pagadas', 'prestamos_outstanding',
    'cuotas_mora', 'cuotas_tarde', 'cuotas_pendientes',
    'dias_promedio', 'max_dias_mora'
]
for col in numeric_columns:
    df_merged[col] = pd.to_numeric(df_merged[col], errors='coerce')

# Step 1: Credit Status
def classify_credit_status(row):
    if row['cuotas_pagadas'] == 0 and row['prestamos_outstanding'] > 0:
        return "Defaulted"
    elif row['cuotas_mora'] >= 3 or row['max_dias_mora'] > 30:
        return "Delinquent"
    elif row['cuotas_tarde'] >= 1 or (0 < row['max_dias_mora'] <= 30):
        return "Late Payments"
    elif row['cuotas_pendientes'] == 0 and row['cuotas_mora'] == 0:
        return "Up to date"
    else:
        return "Unknown"

df_merged['credit_status'] = df_merged.apply(classify_credit_status, axis=1)

# Step 2: Contactability Level
def calculate_contactability(row):
    answered = row['Llamada_Contestada'] if pd.notna(row['Llamada_Contestada']) else 0
    if answered >= 3:
        return "High"
    elif answered in [1, 2]:
        return "Medium"
    else:
        return "Low"

df_merged['contactability_level'] = df_merged.apply(calculate_contactability, axis=1)

# Step 3: Commercial Risk
def calculate_commercial_risk(row):
    riesgo_fraude = bool(row['Fraude_Riesgo']) if pd.notna(row['Fraude_Riesgo']) else False
    llamadas_muchas = bool(row['>10_Llamadas']) if pd.notna(row['>10_Llamadas']) else False
    promesa_pago = bool(row['Promesa_de_Pago']) if pd.notna(row['Promesa_de_Pago']) else False

    if row['prestamo_mora'] > 0 and riesgo_fraude:
        return "Very High"
    elif row['cuotas_mora'] > 0 and llamadas_muchas:
        return "High"
    elif row['cuotas_tarde'] > 0 and promesa_pago:
        return "Medium"
    elif row['cuotas_pagadas'] > 0 and row['cuotas_mora'] == 0:
        return "Low"
    else:
        return "Unknown"

df_merged['commercial_risk'] = df_merged.apply(calculate_commercial_risk, axis=1)

# Step 4: Payment Frequency
def calculate_payment_frequency(row):
    hizo_mas_pagos = bool(row['Hizo_>1_Pago']) if pd.notna(row['Hizo_>1_Pago']) else False
    if hizo_mas_pagos and row['dias_promedio'] <= 30:
        return "Frequent"
    elif not hizo_mas_pagos and row['dias_promedio'] > 30:
        return "Sporadic"
    elif row['cuotas_pagadas'] == 0:
        return "No Payments"
    else:
        return "Other"

df_merged['payment_frequency'] = df_merged.apply(calculate_payment_frequency, axis=1)

# Step 5: Customer Region — CORREGIDO
def normalize(text):
    if pd.isna(text):
        return ""
    text = text.upper()
    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')

region_north  = [normalize(s) for s in ["Nuevo León", "Chihuahua", "Coahuila"]]
region_center = [normalize(s) for s in ["CDMX", "Estado de México", "Querétaro", "Hidalgo", "Puebla"]]
region_south  = [normalize(s) for s in ["Oaxaca", "Chiapas", "Guerrero", "Tabasco", "Veracruz"]]

def assign_region(state):
    state_norm = normalize(state)
    if state_norm in region_north:
        return "North"
    elif state_norm in region_center:
        return "Center"
    elif state_norm in region_south:
        return "South"
    else:
        return "Other"

df_merged['customer_region'] = df_merged['State'].apply(assign_region)

# Step 6: Collection Intensity
def calculate_collection_intensity(total_calls):
    total = total_calls if pd.notna(total_calls) else 0
    if total > 15:
        return "Very High"
    elif 6 <= total <= 15:
        return "Moderate"
    else:
        return "Low"

df_merged['collection_intensity'] = df_merged['Total_Llamadas'].apply(calculate_collection_intensity)

# Step 7: Effective Payer
def is_effective_payer(row):
    promesa       = bool(row['Promesa_de_Pago']) if pd.notna(row['Promesa_de_Pago']) else False
    pago_realizado = bool(row['Ya Pagaron'])     if pd.notna(row['Ya Pagaron'])     else False
    return "Yes" if promesa and pago_realizado else "No"

df_merged['effective_payer'] = df_merged.apply(is_effective_payer, axis=1)

# Step 8: Target Variable – Intensive Use (assigned correctly)
def classify_intensive_use(row):
    return int(
        (row.get('prestamos_totales', 0) >= 4) or
        (row.get('prestamos_outstanding', 0) >= 2) or
        (row.get('dias_promedio', np.inf) <= 15)
    )

df_merged['intensive_use'] = df_merged.apply(classify_intensive_use, axis=1)

df_merged.isna().sum(),df_merged.shape

df_merged["Fraude_Riesgo"] = df_merged["Fraude_Riesgo"].fillna("No_fraude")

df_merged["Fraude_Riesgo"] = df_merged["Fraude_Riesgo"].apply(
    lambda x: 1 if str(x).strip().lower() == "fraude_riesgo" else 0
).astype("int")

df_merged["Fraude_Riesgo"].value_counts()

df_aliadas_team2["ultimo_pago"] = pd.to_datetime(df_aliadas_team2["ultimo_pago"], errors="coerce").dt.strftime('%d/%m/%Y %H:%M:%S')
#df_aliadas_team2["Cosecha"] = pd.to_datetime(df_aliadas_team2["ultimo_pago"], errors="coerce").dt.strftime('%d/%m/%Y %H:%M:%S')

df_merged['IssueDate'] = pd.to_datetime(df_merged['IssueDate'], format='%d/%m/%Y %H:%M:%S', errors='coerce')
df_merged['IssueMonth'] = df_merged['IssueDate'].dt.month
df_merged['IssueDay'] = df_merged['IssueDate'].dt.day
df_merged['IssueWeekday'] = df_merged['IssueDate'].dt.weekday
df_merged['IssueYear'] = df_merged['IssueDate'].dt.year

df_merged['IssueMonth'] = df_merged['IssueMonth'].astype('int64')
df_merged['IssueDay'] = df_merged['IssueDay'].astype('int64')
df_merged['IssueWeekday'] = df_merged['IssueWeekday'].astype('int64')
df_merged['IssueYear'] = df_merged['IssueYear'].astype('int64')

df_merged = df_merged.dropna(subset=['DisbursementMeans', 'cuotas_tarde'])

cols_to_drop = ['State','City', 'Ally_ID_x','']

df_merged = df_merged.drop(columns=[col for col in cols_to_drop if col in df_merged.columns])

numeric_cols = df_merged.select_dtypes(include=['float64', 'int64']).columns.tolist()

numeric_cols_to_int = [col for col in numeric_cols if col != 'dias_promedio']
for col in numeric_cols_to_int:
    df_merged[col] = df_merged[col].astype(int)

df_merged.dtypes

df_merged.isna().sum(),df_merged.shape

df_merged.head()

# Step 1: Feature selection for clustering
clustering_features = [
    'cuotas_pagadas', 'cuotas_tarde', 'cuotas_mora',
    'dias_promedio', 'Total_Llamadas'
]

df_clustering = df_merged[clustering_features].dropna()

# Step 2: Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_clustering)

# Step 3: Elbow Method
inertia = []
for k in range(1, 11):
    km = KMeans(n_clusters=k, init='k-means++', random_state=42)
    km.fit(X_scaled)
    inertia.append(km.inertia_)

plt.figure()
plt.plot(range(1, 11), inertia, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.grid()
plt.show()

# Step 4: Apply KMeans (based on elbow result, here k=3)
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)
df_merged['cluster'] = kmeans.fit_predict(X_scaled)

# Step 5: PCA for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df_merged['cluster'], cmap='viridis', alpha=0.6)
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('Allies Cluster Visualization (PCA)')
plt.colorbar(label='Cluster')
plt.grid()
plt.show()

# Cluster analysis: summarize key variables per cluster
cluster_summary = df_merged.groupby('cluster')[
    ['cuotas_pagadas', 'cuotas_tarde', 'cuotas_mora', 'dias_promedio', 'Total_Llamadas']
].mean().round(2)

# Add contextual info: region distribution per cluster
region_distribution = df_merged.groupby('cluster')['customer_region'].value_counts(normalize=True).unstack().fillna(0).round(2)

# Combine both into one view
summary_with_region = cluster_summary.copy()
for region in region_distribution.columns:
    summary_with_region[f'Region_{region}_%'] = region_distribution[region] * 100

summary_with_region

# Step 1: Define input (X) and target (y)
leakage_cols = [
    'Aliada_ID', 'ID Aliada', 'Ally_ID', 'Ally_ID_y',
    'LoanID', 'ultimo_pago', 'effective_payer',
    'Promesa_de_Pago', 'Ya Pagaron'
]

X = df_merged.drop(columns=leakage_cols + ['intensive_use'], errors='ignore')
y = df_merged['intensive_use']

# Step 2: Identify column types
num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_cols = X.select_dtypes(include='object').columns.tolist()

# Step 3: Define models
cv_models = {
    'DecisionTree': DecisionTreeClassifier(random_state=42),
    'SVC': SVC(kernel='linear', random_state=42),
    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
    'RandomForest': RandomForestClassifier(random_state=42),
    'MLPClassifier': MLPClassifier(max_iter=500, random_state=42)
}

# Step 4: Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_results = []

for name, model in cv_models.items():
    train_scores = []
    test_scores = []

    for train_idx, test_idx in cv.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        pipeline = Pipeline(steps=[
            ('preprocessor', ColumnTransformer(transformers=[
                ('num', StandardScaler(), num_cols),
                ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
            ])),
            ('classifier', model)
        ])

        pipeline.fit(X_train, y_train)

        y_train_pred = pipeline.predict(X_train)
        y_test_pred = pipeline.predict(X_test)

        train_scores.append(balanced_accuracy_score(y_train, y_train_pred))
        test_scores.append(balanced_accuracy_score(y_test, y_test_pred))

    cv_results.append({
        'Model': name,
        'Train Mean': np.mean(train_scores),
        'Train Std': np.std(train_scores),
        'Test Mean': np.mean(test_scores),
        'Test Std': np.std(test_scores)
    })

cv_df = pd.DataFrame(cv_results).sort_values(by='Test Mean', ascending=False)
st.dataframe(cv_df)

# Define metrics to plot
metrics = ['balanced accuracy'] # You can add other metrics here if needed, e.g., ['balanced accuracy', 'accuracy', 'precision', 'recall', 'f1']

# Get the names of the models from the cv_models dictionary
model_names = list(cv_models.keys())

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))
axes = axes.flatten()

# Assuming you want to plot up to 4 metrics if defined in the 'metrics' list
for i, metric in enumerate(metrics[:len(axes)]): # Iterate only up to the number of available subplots
    metric_data = []

    # Define the scoring function for the current metric
    if metric == 'balanced accuracy':
        scorer = make_scorer(balanced_accuracy_score)
    elif metric == 'accuracy':
         scorer = make_scorer(accuracy_score)
    elif metric == 'precision':
         scorer = make_scorer(precision_score)
    elif metric == 'recall':
         scorer = make_scorer(recall_score)
    elif metric == 'f1':
         scorer = make_scorer(f1_score)
    else:
        print(f"Warning: Unknown metric '{metric}'. Skipping.")
        continue # Skip to the next metric if unknown

    for model_name in model_names:
        pipeline = Pipeline(steps=[
            ('preprocessor', ColumnTransformer(transformers=[
                ('num', StandardScaler(), num_cols),
                ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
            ])),
            ('classifier', cv_models[model_name])
        ])

        # Use the defined scorer for cross_val_score
        scores = cross_val_score(pipeline, X, y, cv=cv, scoring=scorer)
        metric_data.append(scores)

    # Plot the boxplot on the current axis
    sns.boxplot(data=metric_data, ax=axes[i])
    axes[i].set_title(f'{metric.replace("_", " ").title()} by Model') # Improve title formatting
    axes[i].set_xticks(range(len(model_names)))
    axes[i].set_xticklabels(model_names)
    axes[i].set_ylabel(metric.replace("_", " ").title())

# Turn off any unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])


plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
sns.countplot(x='intensive_use', data=df_merged)
plt.title('Class Balance of Intensive Use')
plt.xlabel('Intensive Use (0: No, 1: Yes)')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['No', 'Yes'])
plt.show()

# Step 1: Reuse preprocessor
preprocessor = ColumnTransformer(transformers=[
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
])

# Step 2: Apply preprocessing
X_processed = preprocessor.fit_transform(X)

# Step 3: Apply SMOTE
smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X_processed, y)

# Step 4: Train-test split AFTER SMOTE (balanced data)
X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(
    X_smote, y_smote, test_size=0.2, stratify=y_smote, random_state=42
)

# Step 5: Confirm shapes and balance
print("After SMOTE:")
print("Training set shape:", X_train_smote.shape, y_train_smote.shape)
print("Test set shape:", X_test_smote.shape, y_test_smote.shape)
print("Class distribution (train):", pd.Series(y_train_smote).value_counts())
print("Class distribution (test):", pd.Series(y_test_smote).value_counts())

# Suprimir warnings de OneHotEncoder por categorías nuevas
warnings.filterwarnings("ignore", message="Found unknown categories*")

# Asegurar que y es Series
y = pd.Series(y)

# Definir CV
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Guardar resultados
cv_results_smote_detailed = []

for name, model in cv_models.items():
    train_scores = []
    test_scores = []

    for train_idx, test_idx in cv.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        pipeline = ImbPipeline(steps=[
            ('preprocessor', ColumnTransformer(transformers=[
                ('num', StandardScaler(), num_cols),
                ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
            ])),
            ('smote', SMOTE(random_state=42)),
            ('classifier', model)
        ])

        pipeline.fit(X_train, y_train)

        y_train_pred = pipeline.predict(X_train)
        y_test_pred = pipeline.predict(X_test)

        train_scores.append(balanced_accuracy_score(y_train, y_train_pred))
        test_scores.append(balanced_accuracy_score(y_test, y_test_pred))

    cv_results_smote_detailed.append({
        'Model': name,
        'Train Mean': np.mean(train_scores),
        'Train Std': np.std(train_scores),
        'Test Mean': np.mean(test_scores),
        'Test Std': np.std(test_scores)
    })

# Mostrar resultados ordenados por rendimiento en test
cv_df_smote_detailed = pd.DataFrame(cv_results_smote_detailed).sort_values(by='Test Mean', ascending=False)
st.dataframe(cv_df_smote_detailed)

# Define metrics
metrics = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score)
}

# Plot setup
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))
axes = axes.flatten()

# Boxplots for each metric
for i, (metric_name, scorer) in enumerate(metrics.items()):
    metric_data = []

    for model_name, model in cv_models.items():
        pipeline = ImbPipeline(steps=[
            ('preprocessor', ColumnTransformer(transformers=[
                ('num', StandardScaler(), num_cols),
                ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
            ])),
            ('smote', SMOTE(random_state=42)),
            ('classifier', model)
        ])

        scores = cross_val_score(pipeline, X, y, cv=cv, scoring=scorer)
        metric_data.append(scores)

    sns.boxplot(data=metric_data, ax=axes[i])
    axes[i].set_title(f'{metric_name.capitalize()} with SMOTE')
    axes[i].set_xticks(range(len(cv_models)))  # ✔️ Establece ticks
    axes[i].set_xticklabels(list(cv_models.keys()))  # ✔️ Ahora puedes usar los labels
    axes[i].set_ylabel(metric_name.capitalize())

plt.tight_layout()
plt.show()

splits = [0.2, 0.25, 0.3]
results = []

from imblearn.pipeline import Pipeline as ImbPipeline

results_smote = []  # Nueva lista de resultados con SMOTE

for test_size in splits:
    print(f"\n=================== Random Forest with SMOTE - Test Size {test_size} ===================")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, stratify=y, random_state=42
    )

    preprocessor = ColumnTransformer([
        ('num', StandardScaler(), num_cols),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
    ])

    pipeline_rf_smote = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', SMOTE(random_state=42)),
        ('classifier', RandomForestClassifier(random_state=42))
    ])

    param_grid_rf = {
        'classifier__n_estimators': [50, 100],
        'classifier__max_depth': [4, 5, 10],
        'classifier__min_samples_split': [2, 5],
        'classifier__min_samples_leaf': [5, 10]
    }

    grid_rf = GridSearchCV(
        pipeline_rf_smote,
        param_grid_rf,
        scoring='balanced_accuracy',
        cv=5,
        n_jobs=-1
    )
    grid_rf.fit(X_train, y_train)

    print(f"✅ Best RandomForest Params (SMOTE): {grid_rf.best_params_}")

    y_proba_rf = grid_rf.predict_proba(X_test)[:, 1]
    auc_rf = roc_auc_score(y_test, y_proba_rf)
    print(f"AUC (RF): {auc_rf:.4f}")

    fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_proba_rf)
    optimal_idx_rf = (tpr_rf - fpr_rf).argmax()
    optimal_threshold_rf = thresholds_rf[optimal_idx_rf]
    y_pred_rf = (y_proba_rf >= optimal_threshold_rf).astype(int)

    plt.figure()
    plt.plot(fpr_rf, tpr_rf, label=f'AUC = {auc_rf:.4f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC Curve - Random Forest with SMOTE (Test size {test_size})")
    plt.legend()
    plt.grid()
    plt.show()

    print("📊 Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred_rf))
    print("\n📋 Classification Report:")
    print(classification_report(y_test, y_pred_rf))

    results_smote.append({
        'Model': 'RandomForest + SMOTE',
        'Test Size': test_size,
        'AUC': auc_rf,
        'Accuracy': accuracy_score(y_test, y_pred_rf),
        'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred_rf),
        'Precision': precision_score(y_test, y_pred_rf),
        'Recall': recall_score(y_test, y_pred_rf),
        'F1-score': f1_score(y_test, y_pred_rf)
    })

for test_size in splits:
    print(f"\n=================== Logistic Regression with SMOTE - Test Size {test_size} ===================")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, stratify=y, random_state=42
    )

    preprocessor = ColumnTransformer([
        ('num', StandardScaler(), num_cols),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
    ])

    pipeline_lr_smote = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', SMOTE(random_state=42)),
        ('classifier', LogisticRegression(solver='liblinear', random_state=42))
    ])

    param_grid_lr = {
        'classifier__C': [0.01, 0.1, 1, 10],
        'classifier__penalty': ['l1', 'l2']
    }

    grid_lr = GridSearchCV(
        pipeline_lr_smote,
        param_grid_lr,
        scoring='balanced_accuracy',
        cv=5,
        n_jobs=-1
    )
    grid_lr.fit(X_train, y_train)

    print(f"✅ Best LogisticRegression Params (SMOTE): {grid_lr.best_params_}")

    y_proba_lr = grid_lr.predict_proba(X_test)[:, 1]
    auc_lr = roc_auc_score(y_test, y_proba_lr)
    print(f"AUC (LR): {auc_lr:.4f}")

    fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_proba_lr)
    optimal_idx_lr = (tpr_lr - fpr_lr).argmax()
    optimal_threshold_lr = thresholds_lr[optimal_idx_lr]
    y_pred_lr = (y_proba_lr >= optimal_threshold_lr).astype(int)

    plt.figure()
    plt.plot(fpr_lr, tpr_lr, label=f'AUC = {auc_lr:.4f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC Curve - Logistic Regression with SMOTE (Test size {test_size})")
    plt.legend()
    plt.grid()
    plt.show()

    print("📊 Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred_lr))
    print("\n📋 Classification Report:")
    print(classification_report(y_test, y_pred_lr))

    results_smote.append({
        'Model': 'LogisticRegression + SMOTE',
        'Test Size': test_size,
        'AUC': auc_lr,
        'Accuracy': accuracy_score(y_test, y_pred_lr),
        'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred_lr),
        'Precision': precision_score(y_test, y_pred_lr),
        'Recall': recall_score(y_test, y_pred_lr),
        'F1-score': f1_score(y_test, y_pred_lr)
    })

# Mostrar resumen de ambos modelos
results_smote_df = pd.DataFrame(results_smote)
st.dataframe(results_smote_df)

# 1. Selección de variables ampliadas
expanded_features = [
    # Demand Behaviour
    'dias_promedio', 'prestamos_outstanding', 'prestamo_mora',
    'cuotas_mora', 'cuotas_tarde', 'cuotas_pagadas',
    'max_dias_mora', 'dias_mora', 'Hizo_>1_Pago',
    'Total_Llamadas', '>10_Llamadas', 'TasaMora',

    # Basic Demand
    'prestamos_totales', 'LoanAmount', 'LoanType',
    'DisbursementMeans', 'RecipientType',

    # Contexto estratégico
    'customer_region'
]

# 2. Definir X (features) e y (target)
X = df_merged[expanded_features]
y = df_merged['intensive_use']

# 3. Separar numéricas y categóricas
num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_cols = X.select_dtypes(include=['object']).columns.tolist()

# 4. Pipeline de preprocesamiento
preprocessor = ColumnTransformer(transformers=[
    ('num', Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ]), num_cols),
    ('cat', Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ]), cat_cols)
])

# 5. Modelo sin regularización
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000))
])

# 6. Entrenar modelo
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)
model.fit(X_train, y_train)

# 7. Obtener nombres de features después del OneHotEncoder
ohe = model.named_steps['preprocessor'].named_transformers_['cat']['onehot']
encoded_cat_features = ohe.get_feature_names_out(cat_cols)
all_features = np.concatenate([num_cols, encoded_cat_features])

# 8. Extraer coeficientes
coefficients = model.named_steps['classifier'].coef_[0]

# 9. Crear DataFrame de importancia
importances_df = pd.DataFrame({
    'Feature': all_features,
    'Coefficient': coefficients,
    'Abs_Coefficient': np.abs(coefficients)
}).sort_values(by='Abs_Coefficient', ascending=False)

# 10. Mostrar top 15
importances_df.reset_index(drop=True, inplace=True)
st.dataframe(importances_df.head(15))

# 1. Definir variables importantes
top_5_features = ['dias_promedio', 'RecipientType', 'LoanType', 'DisbursementMeans', 'customer_region']
num_cols = ['dias_promedio']
cat_cols = ['RecipientType', 'LoanType', 'DisbursementMeans', 'customer_region']

X_raw = df_merged[top_5_features]
y_raw = df_merged['intensive_use']

# 2. Preprocesamiento + SMOTE antes del split
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
])

X_processed = preprocessor.fit_transform(X_raw)

smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X_processed, y_raw)

# Confirmar balanceo
print("✅ Distribución después de SMOTE:")
print(pd.Series(y_smote).value_counts())

# 3. GridSearch + ROC por cada test size
splits = [0.2, 0.25, 0.3]
results_smote_presplit = []

for test_size in splits:
    print(f"\n================ Logistic Regression (SMOTE before split) - Test Size {test_size} =================")

    X_train, X_test, y_train, y_test = train_test_split(
        X_smote, y_smote, test_size=test_size, stratify=y_smote, random_state=42
    )

    model = LogisticRegression(solver='liblinear', random_state=42)

    param_grid = {
        'C': [0.01, 0.1, 1, 10],
        'penalty': ['l1', 'l2']
    }

    grid = GridSearchCV(model, param_grid, scoring='balanced_accuracy', cv=5, n_jobs=-1)
    grid.fit(X_train, y_train)

    print(f"✅ Best parameters: {grid.best_params_}")

    y_proba = grid.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_proba)
    print(f"AUC: {auc:.4f}")

    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    optimal_idx = (tpr - fpr).argmax()
    optimal_threshold = thresholds[optimal_idx]
    y_pred = (y_proba >= optimal_threshold).astype(int)

    # ROC curve
    plt.figure()
    plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC - Logistic Regression (SMOTE before split) - Test size {test_size}")
    plt.legend()
    plt.grid()
    plt.show()

    print("📊 Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\n📋 Classification Report:")
    print(classification_report(y_test, y_pred))

    results_smote_presplit.append({
        'Model': 'LogReg + SMOTE before split',
        'Test Size': test_size,
        'AUC': auc,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-score': f1_score(y_test, y_pred)
    })

# Mostrar tabla resumen final
results_smote_presplit_df = pd.DataFrame(results_smote_presplit)
st.dataframe(results_smote_presplit_df)

# 1. Definir variables importantes
top_5_features = ['dias_promedio', 'RecipientType', 'LoanType', 'DisbursementMeans', 'customer_region']
num_cols = ['dias_promedio']
cat_cols = ['RecipientType', 'LoanType', 'DisbursementMeans', 'customer_region']

X_raw = df_merged[top_5_features]
y_raw = df_merged['intensive_use']

# 2. Preprocesamiento (antes del split)
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
])

X_processed = preprocessor.fit_transform(X_raw)

# 3. GridSearch + ROC para diferentes test sizes
splits = [0.2, 0.25, 0.3]
results_smote_postsplit = []

for test_size in splits:
    print(f"\n================ Logistic Regression (SMOTE after split) - Test Size {test_size} =================")

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y_raw, test_size=test_size, stratify=y_raw, random_state=42
    )

    # SMOTE SOLO en el entrenamiento
    smote = SMOTE(random_state=42)
    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

    # Modelo + GridSearch
    model = LogisticRegression(solver='liblinear', random_state=42)
    param_grid = {
        'C': [0.01, 0.1, 1, 10],
        'penalty': ['l1', 'l2']
    }
    grid = GridSearchCV(model, param_grid, scoring='balanced_accuracy', cv=5, n_jobs=-1)
    grid.fit(X_train_res, y_train_res)

    print(f"✅ Best parameters: {grid.best_params_}")

    # Predicción
    y_proba = grid.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_proba)
    print(f"AUC: {auc:.4f}")

    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    optimal_idx = (tpr - fpr).argmax()
    optimal_threshold = thresholds[optimal_idx]
    y_pred = (y_proba >= optimal_threshold).astype(int)

    # ROC curve
    plt.figure()
    plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC - Logistic Regression (SMOTE after split) - Test size {test_size}")
    plt.legend()
    plt.grid()
    plt.show()

    print("📊 Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\n📋 Classification Report:")
    print(classification_report(y_test, y_pred))

    results_smote_postsplit.append({
        'Model': 'LogReg + SMOTE after split',
        'Test Size': test_size,
        'AUC': auc,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-score': f1_score(y_test, y_pred)
    })

# Mostrar resumen
results_smote_postsplit_df = pd.DataFrame(results_smote_postsplit)
st.dataframe(results_smote_postsplit_df)

# 2. Crear matriz de confusión
cm = confusion_matrix(y_test, y_pred)

# 3. Graficar
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['No Intensivo', 'Intensivo'],
            yticklabels=['No Intensivo', 'Intensivo'])
plt.xlabel('Predicción')
plt.ylabel('Valor Real')
plt.title('Matriz de Confusión - Logistic Regression (Test Size 0.30)')
plt.show()



"""## Graphic Insights"""

from bokeh.plotting import figure, show, output_notebook
from bokeh.models import ColumnDataSource, HoverTool, Legend, LinearColorMapper, ColorBar, BasicTicker, PrintfTickFormatter, FactorRange
from bokeh.palettes import Category10, RdBu, Category20, Viridis256, linear_palette
from bokeh.palettes import Set2_8
from bokeh.io import output_notebook
from bokeh.transform import transform, factor_cmap
from scipy.stats import gaussian_kde
from bokeh.models import LabelSet

from bokeh.io import output_notebook, show
from bokeh.plotting import figure
import pandas as pd


from scipy.stats import gaussian_kde

# Total de préstamos por aliada
basic_demand = df_merged.groupby("ID Aliada")["LoanID"].count().reset_index()
basic_demand.columns = ["ID Aliada", "basic_demand"]

plt.figure(figsize=(10, 6))
sns.histplot(basic_demand['basic_demand'], bins=20, kde=True)
plt.title('Distribution of Basic Demand (Total Loans per Ally)')
plt.xlabel('Number of Loans')
plt.ylabel('Number of Allies')
plt.grid(axis='y')
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Calcular demanda básica por Aliada
basic_demand = df.groupby("ID Aliada")["LoanID"].count().reset_index()
basic_demand.columns = ["ID Aliada", "basic_demand"]

# Histograma
hist, edges = np.histogram(basic_demand["basic_demand"], bins=20)

# Curva KDE
kde = gaussian_kde(basic_demand["basic_demand"])
x_kde = np.linspace(basic_demand["basic_demand"].min(), basic_demand["basic_demand"].max(), 200)
y_kde = kde(x_kde) * len(basic_demand) * (edges[1] - edges[0])

# Fuentes de datos para Bokeh
source_hist = ColumnDataSource(data=dict(top=hist, left=edges[:-1], right=edges[1:]))
source_kde = ColumnDataSource(data=dict(x=x_kde, y=y_kde))

# Crear figura con fondo transparente
p = figure(title="Distribución de la Demanda Básica (Total de Préstamos por Aliada)",
           x_axis_label="Número de Préstamos",
           y_axis_label="Número de Aliadas",
           width=800, height=400,
           tools="pan,box_zoom,reset,save",
           background_fill_color=None,
           border_fill_color=None)

# Histograma en dorado
p.quad(top='top', bottom=0, left='left', right='right', source=source_hist,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9, legend_label="Histograma")

# KDE en rojo elegante
p.line('x', 'y', source=source_kde, line_color="#e63946", line_width=3, legend_label="Curva KDE")

# Hover estilizado
hover = HoverTool(tooltips=[
    ("Rango", "@left{0} - @right{0}"),
    ("Cantidad", "@top")
])
p.add_tools(hover)

# Tipografía elegante
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"

p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"

p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Leyenda elegante
p.legend.label_text_font = "helvetica"
p.legend.label_text_color = "#1c1c1c"
p.legend.background_fill_alpha = 0.0
p.legend.border_line_color = None
p.legend.location = "top_right"

# Grid sutil
p.grid.grid_line_color = "#cccccc"
p.grid.grid_line_alpha = 0.3
p.outline_line_color = None

# Mostrar gráfica
show(p)

plt.figure(figsize=(10, 6))
sns.boxplot(x=basic_demand['basic_demand'])
plt.title('Boxplot of Basic Demand (Total Loans per Ally)')
plt.xlabel('Number of Loans')
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Calcular demanda básica por Aliada
basic_demand = df.groupby("ID Aliada")["LoanID"].count().reset_index()
basic_demand.columns = ["ID Aliada", "basic_demand"]
data = basic_demand["basic_demand"]

# Calcular cuartiles
q1 = data.quantile(0.25)
q2 = data.quantile(0.50)
q3 = data.quantile(0.75)
iqr = q3 - q1
upper = q3 + 1.5 * iqr
lower = q1 - 1.5 * iqr

# Outliers
outliers = data[(data < lower) | (data > upper)]

# Crear figura con fondo transparente
p = figure(title="Boxplot de la Demanda Básica (Préstamos Totales por Aliada)",
           width=800, height=300, x_axis_label="Número de Préstamos",
           y_range=["Basic Demand"],
           tools="pan,box_zoom,reset,save",
           background_fill_color=None,
           border_fill_color=None)

# Caja dorada
p.hbar(y=["Basic Demand"], height=0.4, left=q1, right=q3,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Mediana en rojo
p.segment(x0=q2, y0=0.8, x1=q2, y1=1.2, line_color="#e63946", line_width=2)

# Bigotes
p.segment(x0=lower, y0=1, x1=q1, y1=1, line_color="#aaaaaa")
p.segment(x0=q3, y0=1, x1=upper, y1=1, line_color="#aaaaaa")

# Outliers en negro más visibles
p.scatter(x=outliers, y=["Basic Demand"] * len(outliers), size=8, marker="circle",
          fill_alpha=0.9, fill_color="#000000", line_color=None)

# Estética premium Stratify
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Grid sutil
p.grid.grid_line_color = "#cccccc"
p.grid.grid_line_alpha = 0.3
p.outline_line_color = None

# Mostrar gráfico
show(p)

# 1. Extract Year and Month
df_merged["Year"] = df_merged["IssueDate"].dt.year
df_merged["Month"] = df_merged["IssueDate"].dt.strftime('%B') # Extract month name (e.g., January, February)

# 2. Group by Year and Month
seasonality_by_year = df_merged.groupby(["Year", "Month"])["LoanID"].count().reset_index()
seasonality_by_year.columns = ["Year", "Month", "MonthlyDemand"]

# Ensure consistent month ordering for each year
ordered_months = ["January", "February", "March", "April", "May", "June",
                  "July", "August", "September", "October", "November", "December"]
seasonality_by_year["Month"] = pd.Categorical(seasonality_by_year["Month"], categories=ordered_months, ordered=True)
seasonality_by_year = seasonality_by_year.sort_values(["Year", "Month"])

# 3. Filter for each year
seasonality_2024 = seasonality_by_year[seasonality_by_year["Year"] == 2024].reset_index(drop=True)
seasonality_2025 = seasonality_by_year[seasonality_by_year["Year"] == 2025].reset_index(drop=True)

# Filter the data to include only 'CONCORD' and 'Mercado Abierto'
concord_mercado_data = df_merged[df_merged['canal_ajustado'].isin(['CONCORD', 'Mercado Abierto'])].copy()

# Ensure 'IssueDate' is datetime and extract Year and Month
concord_mercado_data['IssueDate'] = pd.to_datetime(concord_mercado_data['IssueDate'], errors='coerce')
concord_mercado_data = concord_mercado_data.dropna(subset=['IssueDate']) # Drop rows where date conversion failed
concord_mercado_data["Year"] = concord_mercado_data["IssueDate"].dt.year
concord_mercado_data["Month"] = concord_mercado_data["IssueDate"].dt.strftime('%B')

# Group by Year, Month, and canal_ajustado to get loan counts
canal_monthly_demand = concord_mercado_data.groupby(["Year", "Month", "canal_ajustado"])["LoanID"].count().reset_index()
canal_monthly_demand.columns = ["Year", "Month", "Canal", "MonthlyDemand"]

# Ensure consistent month ordering for each year
ordered_months = ["January", "February", "March", "April", "May", "June",
                  "July", "August", "September", "October", "November", "December"]
canal_monthly_demand["Month"] = pd.Categorical(canal_monthly_demand["Month"], categories=ordered_months, ordered=True)
canal_monthly_demand = canal_monthly_demand.sort_values(["Year", "Month"])

# Filter for the desired period (March 2024 to February 2025)
canal_seasonality_plot_data = pd.concat([
    canal_monthly_demand[(canal_monthly_demand["Year"] == 2024) & (canal_monthly_demand["Month"].isin(ordered_months[2:]))], # Mar-Dec 2024
    canal_monthly_demand[(canal_monthly_demand["Year"] == 2025) & (canal_monthly_demand["Month"].isin(ordered_months[:2]))]  # Jan-Feb 2025
])

# Create a combined 'Year-Month' string for the x-axis
canal_seasonality_plot_data["Year-Month"] = canal_seasonality_plot_data["Year"].astype(str) + "-" + canal_seasonality_plot_data["Month"].astype(str)

# Define the desired order for plotting (Mar 2024 -> Feb 2025)
plot_order = []
for month in ordered_months[2:]:
    plot_order.append(f"2024-{month}")
for month in ordered_months[:2]:
    plot_order.append(f"2025-{month}")

# Ensure the 'Year-Month' column is ordered correctly for plotting
canal_seasonality_plot_data["Year-Month"] = pd.Categorical(canal_seasonality_plot_data["Year-Month"], categories=plot_order, ordered=True)
canal_seasonality_plot_data = canal_seasonality_plot_data.sort_values("Year-Month")

# Calculate the total monthly demand across the selected channels for the trendline
total_monthly_demand_concord_mercado = canal_seasonality_plot_data.groupby("Year-Month")["MonthlyDemand"].sum().reset_index()
total_monthly_demand_concord_mercado.columns = ["Year-Month", "TotalMonthlyDemand"]

# Create the combined plot
plt.figure(figsize=(14, 7))

# Bar plot for the breakdown by channel
bar_plot = sns.barplot(
    data=canal_seasonality_plot_data,
    x="Year-Month",
    y="MonthlyDemand",
    hue="Canal",
    dodge=False,  # Stack the bars
    palette='viridis'
)

# Add the line plot for the total monthly demand
line_plot = sns.lineplot(
    data=total_monthly_demand_concord_mercado,
    x="Year-Month",
    y="TotalMonthlyDemand",
    marker='o',
    color='red',  # Set the line color
    label='Total Monthly Demand' # Label for the legend
)

# Customize the plot
plt.title("Monthly Loan Demand by Channel (CONCORD, Mercado Abierto) with Total Trendline (March 2024 - February 2025)")
plt.xlabel("Month")
plt.ylabel("Number of Loans")
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', linestyle='--')
plt.legend(title="Channel", loc='upper left') # Add legend for both bars and line
plt.tight_layout()
plt.show()

output_notebook()

# Colores elegantes
channel_colors = {
    "CONCORD": "#cba660",
    "Mercado Abierto": "#4a4a4a"
}

canales = canal_seasonality_plot_data["Canal"].unique()

# Figura con mayor ancho y espacio en eje X
p = figure(x_range=plot_order, height=450, width=1100,
           title="Demanda Mensual de Préstamos por Canal (CONCORD, Mercado Abierto) con Línea de Tendencia",
           x_axis_label="Mes", y_axis_label="Número de Préstamos",
           background_fill_color=None, border_fill_color=None,
           toolbar_location=None,
           min_border_left=50, min_border_right=50)

# Barras por canal
for canal in canales:
    canal_data = canal_seasonality_plot_data[canal_seasonality_plot_data["Canal"] == canal]
    source = ColumnDataSource(data=dict(
        x=canal_data["Year-Month"],
        y=canal_data["MonthlyDemand"]
    ))
    p.vbar(x='x', top='y', width=0.8, source=source,
           color=channel_colors.get(canal, "#888888"),
           legend_label=canal, muted_alpha=0.2)

# Línea de tendencia total
source_line = ColumnDataSource(total_monthly_demand_concord_mercado)
p.line(x='Year-Month', y='TotalMonthlyDemand', source=source_line,
       line_color='#e63946', line_width=2, legend_label="Demanda Total")

# Puntos sobre la línea (compatible Bokeh 3.4+)
p.scatter(x='Year-Month', y='TotalMonthlyDemand', source=source_line,
          size=6, marker="circle", fill_color='#e63946',
          line_color=None, legend_label="Demanda Total")

# Rotar etiquetas del eje X
p.xaxis.major_label_orientation = pi / 3

# Estética premium
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Grid sobrio
p.xgrid.grid_line_dash = 'dashed'
p.grid.grid_line_color = "#cccccc"
p.grid.grid_line_alpha = 0.3
p.outline_line_color = None

# Leyenda refinada
p.legend.location = "top_left"
p.legend.label_text_font = "helvetica"
p.legend.label_text_color = "#1c1c1c"
p.legend.background_fill_alpha = 0.0
p.legend.title = "Canal"
p.legend.click_policy = "mute"

# Mostrar
show(p)

df_merged["TasaMora"] = df_merged["cuotas_mora"] / df_merged["cuotas_pagadas"]
mora_por_aliada = df_merged.groupby("ID Aliada")["TasaMora"].mean().sort_values(ascending=False)

variables = ["cuotas_pagadas", "cuotas_mora", "cuotas_tarde", "dias_promedio", "max_dias_mora"]
sns.heatmap(df_merged[variables].corr(), annot=True, cmap="coolwarm")

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")
df["TasaMora"] = df["cuotas_mora"] / df["cuotas_pagadas"]

# Variables seleccionadas
variables = ["cuotas_pagadas", "cuotas_mora", "cuotas_tarde", "dias_promedio", "max_dias_mora"]
corr_matrix = df[variables].corr().round(2)
corr_long = corr_matrix.stack().reset_index()
corr_long.columns = ["var1", "var2", "correlation"]

source = ColumnDataSource(corr_long)

# Paleta Stratify personalizada (sin interpolación)
stratify_palette = ["#e63946", "#f1c27d", "#cba660", "#4a4a4a", "#0E0E0E"]
mapper = LinearColorMapper(palette=stratify_palette, low=-1, high=1)

# Figura con fondo tipo transparente
p = figure(title="Mapa de Correlación de Variables de Riesgo de Mora",
           x_range=variables,
           y_range=list(reversed(variables)),
           x_axis_location="above",
           tools="",
           toolbar_location=None,
           width=600, height=600,
           background_fill_color=None,  # "transparente"
           border_fill_color=None)

# Rectángulos
p.rect(x="var1", y="var2", width=1, height=1,
       source=source,
       line_color=None,
       fill_color=transform('correlation', mapper),
       fill_alpha=1.0)  # asegúrate de que las celdas sí se vean

# Etiquetas internas
label_source = ColumnDataSource(data=dict(
    x=corr_long["var1"],
    y=corr_long["var2"],
    value=corr_long["correlation"].astype(str)
))

labels = LabelSet(x='x', y='y', text='value', source=label_source,
                  text_align='center', text_baseline='middle',
                  text_font="helvetica", text_font_size="10pt",
                  text_color="#f4f4f4")
p.add_layout(labels)

# Colorbar
color_bar = ColorBar(color_mapper=mapper,
                     major_label_text_font_size="10pt",
                     ticker=BasicTicker(desired_num_ticks=10),
                     formatter=PrintfTickFormatter(format="%.2f"),
                     label_standoff=12,
                     location=(0, 0),
                     background_fill_color=None,
                     major_label_text_color="#f4f4f4")

p.add_layout(color_bar, 'right')

# Ejes con estilo premium
p.axis.axis_line_color = None
p.axis.major_tick_line_color = None
p.axis.major_label_text_font = "helvetica"
p.axis.major_label_text_font_size = "10pt"
p.axis.major_label_text_color = "#f4f4f4"
p.axis.major_label_standoff = 0
p.xaxis.major_label_orientation = np.pi / 4
p.outline_line_color = None

# Mostrar (o usar en Streamlit con st.bokeh_chart(p))
show(p)

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

features = df_merged[["cuotas_pagadas", "cuotas_mora", "cuotas_tarde", "dias_promedio"]]
scaled = StandardScaler().fit_transform(features)

kmeans = KMeans(n_clusters=3, random_state=0)
df_merged["Cluster"] = kmeans.fit_predict(scaled)

df_merged["IssueDate"] = pd.to_datetime(df_merged["IssueDate"])
tiempo_promedio = df_merged.groupby("ID Aliada")["IssueDate"].apply(lambda x: x.sort_values().diff().mean().days)

plt.figure(figsize=(10, 6))
sns.histplot(tiempo_promedio.dropna(), bins=30, kde=True)
plt.title('Distribution of Average Time Between Loans per Ally')
plt.xlabel('Average Days Between Loans')
plt.ylabel('Number of Allies')
plt.grid(axis='y')
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Convertir fechas
df["IssueDate"] = pd.to_datetime(df["IssueDate"], errors='coerce')
df = df.dropna(subset=["IssueDate"])

# KMeans clustering
features = df[["cuotas_pagadas", "cuotas_mora", "cuotas_tarde", "dias_promedio"]]
scaled = StandardScaler().fit_transform(features)
kmeans = KMeans(n_clusters=3, random_state=0)
df["Cluster"] = kmeans.fit_predict(scaled)

# Tiempo promedio entre préstamos por Aliada
tiempo_promedio = df.groupby("ID Aliada")["IssueDate"].apply(lambda x: x.sort_values().diff().mean().days)
tiempo_promedio = tiempo_promedio.dropna()

# Histograma
hist, edges = np.histogram(tiempo_promedio, bins=30)

# KDE
kde = gaussian_kde(tiempo_promedio)
x_kde = np.linspace(tiempo_promedio.min(), tiempo_promedio.max(), 200)
y_kde = kde(x_kde) * len(tiempo_promedio) * (edges[1] - edges[0])

# Fuentes
source_hist = ColumnDataSource(data=dict(top=hist, left=edges[:-1], right=edges[1:]))
source_kde = ColumnDataSource(data=dict(x=x_kde, y=y_kde))

# Crear figura con fondo tipo transparente
p = figure(title="Distribución del Tiempo Promedio entre Préstamos por Aliada",
           x_axis_label="Días Promedio entre Préstamos",
           y_axis_label="Número de Aliadas",
           width=800, height=400,
           background_fill_color=None,
           border_fill_color=None)

# Histograma en dorado Stratify
p.quad(top='top', bottom=0, left='left', right='right', source=source_hist,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# KDE rojo elegante
p.line('x', 'y', source=source_kde, line_color="#e63946", line_width=3)

# Hover refinado
hover = HoverTool(tooltips=[
    ("Intervalo", "@left{0} - @right{0} días"),
    ("Cantidad", "@top")
])
p.add_tools(hover)

# Estética de texto y ejes
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"

p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"

p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Grid sutil
p.grid.grid_line_color = "#cccccc"
p.grid.grid_line_alpha = 0.3
p.outline_line_color = None

# Mostrar
show(p)

plt.figure(figsize=(10, 6))
sns.boxplot(x=tiempo_promedio.dropna())
plt.title('Boxplot of Average Time Between Loans per Ally')
plt.xlabel('Average Days Between Loans')
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")
df["IssueDate"] = pd.to_datetime(df["IssueDate"], errors='coerce')
df = df.dropna(subset=["IssueDate"])

# Calcular tiempo promedio entre préstamos por aliada
tiempo_promedio = df.groupby("ID Aliada")["IssueDate"].apply(
    lambda x: x.sort_values().diff().mean().days
).dropna()

# Calcular cuartiles e IQR
q1 = tiempo_promedio.quantile(0.25)
q2 = tiempo_promedio.quantile(0.50)
q3 = tiempo_promedio.quantile(0.75)
iqr = q3 - q1
upper = q3 + 1.5 * iqr
lower = q1 - 1.5 * iqr

# Identificar outliers
outliers = tiempo_promedio[(tiempo_promedio > upper) | (tiempo_promedio < lower)]

# Crear figura con fondo tipo PNG (transparente)
p = figure(title="Boxplot del Tiempo Promedio entre Préstamos por Aliada",
           x_axis_label="Días Promedio entre Préstamos",
           y_range=["Average Loan Interval"],
           width=800, height=300,
           tools="pan,box_zoom,reset,save",
           background_fill_color=None,
           border_fill_color=None)

# Caja en dorado Stratify
p.hbar(y=["Average Loan Interval"], height=0.4, left=q1, right=q3,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Mediana en rojo elegante
p.segment(x0=q2, y0=0.8, x1=q2, y1=1.2, line_color="#e63946", line_width=2)

# Bigotes (whiskers) en gris claro
p.segment(x0=lower, y0=1, x1=q1, y1=1, line_color="#aaaaaa")
p.segment(x0=q3, y0=1, x1=upper, y1=1, line_color="#aaaaaa")

# Outliers en negro, más visibles
p.scatter(x=outliers, y=["Average Loan Interval"] * len(outliers), size=8,
          marker="circle", fill_alpha=0.9, fill_color="#000000", line_color=None)

# Estética premium
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Grid sobrio
p.grid.grid_line_color = "#cccccc"
p.grid.grid_line_alpha = 0.3
p.outline_line_color = None

# Mostrar (o usar con st.bokeh_chart(p))
show(p)

df_merged["ConversionLlamadas"] = df_merged["prestamos_outstanding"] / df_merged["Total_Llamadas"]
conversion_media = df_merged.groupby("ID Aliada")["ConversionLlamadas"].mean()

plt.figure(figsize=(10, 6))
sns.histplot(conversion_media.dropna(), bins=30, kde=True)
plt.title('Distribution of Call-to-Conversion Rate per Ally')
plt.xlabel('Conversion Rate (Loans Outstanding / Total Calls)')
plt.ylabel('Number of Allies')
plt.grid(axis='y')
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Calcular tasa de conversión
df["ConversionLlamadas"] = df["prestamos_outstanding"] / df["Total_Llamadas"].replace(0, np.nan)
conversion_media = df.groupby("ID Aliada")["ConversionLlamadas"].mean().dropna()
conversion_media = conversion_media[np.isfinite(conversion_media)]

# Verificar que haya datos
if not conversion_media.empty:
    # Histograma
    hist, edges = np.histogram(conversion_media, bins=30)

    # KDE
    kde = gaussian_kde(conversion_media)
    x_kde = np.linspace(conversion_media.min(), conversion_media.max(), 200)
    y_kde = kde(x_kde) * len(conversion_media) * (edges[1] - edges[0])

    # Fuentes para Bokeh
    source_hist = ColumnDataSource(data=dict(top=hist, left=edges[:-1], right=edges[1:]))
    source_kde = ColumnDataSource(data=dict(x=x_kde, y=y_kde))

    # Figura con estilo premium
    p = figure(title="Distribución de la Tasa de Conversión de Llamadas por Aliada",
               x_axis_label="Tasa de Conversión (Préstamos / Llamadas Totales)",
               y_axis_label="Número de Aliadas",
               width=800, height=400,
               background_fill_color=None,
               border_fill_color=None,
               tools="pan,box_zoom,reset,save")

    # Histograma en dorado Stratify
    p.quad(top='top', bottom=0, left='left', right='right', source=source_hist,
           fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

    # Línea KDE en rojo elegante
    p.line('x', 'y', source=source_kde, line_color="#e63946", line_width=3)

    # Hover refinado
    hover = HoverTool(tooltips=[
        ("Rango", "@left{0.00} - @right{0.00}"),
        ("Cantidad", "@top")
    ])
    p.add_tools(hover)

    # Estética de texto
    p.title.text_font = "helvetica"
    p.title.text_font_size = "14pt"
    p.title.text_color = "#1c1c1c"
    p.xaxis.axis_label_text_font = "helvetica"
    p.xaxis.axis_label_text_color = "#1c1c1c"
    p.xaxis.major_label_text_font = "helvetica"
    p.xaxis.major_label_text_color = "#333333"
    p.yaxis.axis_label_text_font = "helvetica"
    p.yaxis.axis_label_text_color = "#1c1c1c"
    p.yaxis.major_label_text_font = "helvetica"
    p.yaxis.major_label_text_color = "#333333"

    # Grid limpio
    p.grid.grid_line_color = "#cccccc"
    p.grid.grid_line_alpha = 0.3
    p.outline_line_color = None

    # Mostrar
    show(p)
else:
    print("conversion_media está vacía después del filtrado.")

# prompt: make it a violin plot

plt.figure(figsize=(10, 6))
sns.violinplot(x=basic_demand['basic_demand'])
plt.title('Violin Plot of Basic Demand (Total Loans per Ally)')
plt.xlabel('Number of Loans')
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Calcular demanda básica
basic_demand = df.groupby("ID Aliada")["LoanID"].count().reset_index()
basic_demand.columns = ["ID Aliada", "basic_demand"]

# KDE sobre datos
data = basic_demand["basic_demand"].dropna()
kde = gaussian_kde(data)
x = np.linspace(data.min(), data.max(), 200)
y = kde(x)
y_scaled = y / y.max() * 0.4

# Datos para forma del violín
x_violin = list(x) + list(x[::-1])
y_violin = list(y_scaled) + list(-y_scaled[::-1])
source_violin = ColumnDataSource(data=dict(x=x_violin, y=y_violin))

# Crear figura
p = figure(title="Distribución tipo Violin de la Demanda Básica por Aliada",
           width=800, height=400, x_axis_label="Número de Préstamos",
           y_axis_label="", tools="pan,box_zoom,reset,save",
           background_fill_color=None,
           border_fill_color=None)

# Violín dorado Stratify
p.patch('x', 'y', source=source_violin, fill_color="#cba660",
        fill_alpha=0.9, line_color="#cba660")

# Línea de mediana en rojo
median_val = data.median()
p.line([median_val, median_val], [-0.45, 0.45], color="#e63946", line_width=2, legend_label="Mediana")

# Eje Y limitado y oculto
p.y_range.start = -0.5
p.y_range.end = 0.5
p.yaxis.visible = False

# Estética texto
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"

# Grid y leyenda
p.grid.grid_line_color = "#cccccc"
p.grid.grid_line_alpha = 0.3
p.outline_line_color = None
p.legend.location = "top_right"

# Mostrar
show(p)

# Agrupar cantidad de préstamos por tipo de cliente y tipo de préstamo
loan_distribution = df_merged.groupby(['RecipientType', 'LoanType']).size().reset_index(name='Count')

# Calculate the sum of 'Count' for each 'RecipientType'
recipient_type_counts = loan_distribution.groupby('RecipientType')['Count'].transform('sum')

# Calculate percentage by dividing 'Count' by the total count for each RecipientType
loan_distribution['Percent'] = (loan_distribution['Count'] / recipient_type_counts) * 100

# Tabla resumen de porcentajes
loan_summary_table = loan_distribution.pivot(index='RecipientType', columns='LoanType', values='Percent').fillna(0)
print("📋 Tabla resumen - Porcentaje de tipo de préstamo por tipo de cliente:")
st.dataframe(loan_summary_table)

# Gráfico de barras
plt.figure(figsize=(8, 5))
sns.barplot(data=loan_distribution, x='RecipientType', y='Percent', hue='LoanType')
plt.title("Distribución de Tipo de Préstamo por Tipo de Cliente")
plt.ylabel("Porcentaje (%)")
plt.xlabel("Tipo de Cliente")
plt.legend(title="Tipo de Préstamo")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Agrupar y calcular porcentaje
loan_distribution = df.groupby(['RecipientType', 'LoanType']).size().reset_index(name='Count')
recipient_type_counts = loan_distribution.groupby('RecipientType')['Count'].transform('sum')
loan_distribution['Percent'] = (loan_distribution['Count'] / recipient_type_counts) * 100

loan_distribution['RecipientType'] = loan_distribution['RecipientType'].astype(str)
loan_distribution['LoanType'] = loan_distribution['LoanType'].astype(str)

# Eje X como pares categóricos
x = list(zip(loan_distribution['RecipientType'], loan_distribution['LoanType']))
loan_types = sorted(loan_distribution['LoanType'].unique().tolist())

source = ColumnDataSource(data=dict(
    x=x,
    percent=loan_distribution['Percent'],
    tipo_cliente=loan_distribution['RecipientType'],
    tipo_prestamo=loan_distribution['LoanType']
))

# Paleta Stratify personalizada (ajustable según cantidad de categorías)
stratify_palette = ["#cba660", "#4a4a4a", "#e63946"]  # Dorado, gris oscuro, rojo

# Crear figura
p = figure(x_range=FactorRange(*x), height=400, width=800,
           title="Distribución de Tipos de Préstamo por Tipo de Cliente",
           y_axis_label="Porcentaje (%)", background_fill_color=None,
           border_fill_color=None, toolbar_location=None, tools="")

# Barras coloreadas con Stratify palette
p.vbar(x='x', top='percent', width=0.9, source=source,
       fill_color=factor_cmap('x', palette=stratify_palette,
                              factors=loan_types, start=1))

# HoverTool
hover = HoverTool(tooltips=[
    ("Cliente", "@tipo_cliente"),
    ("Préstamo", "@tipo_prestamo"),
    ("Porcentaje", "@percent{0.2f}%")
])
p.add_tools(hover)

# Estética Stratify
p.xaxis.major_label_orientation = 1
p.xaxis.axis_label = "Tipo de Cliente"
p.y_range.start = 0
p.x_range.range_padding = 0.1
p.ygrid.grid_line_dash = 'dashed'
p.ygrid.grid_line_color = "#cccccc"
p.outline_line_color = None

p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Mostrar gráfico
show(p)

# Agrupación por tipo de cliente
total_loans_by_client = df_merged.groupby('RecipientType')['LoanID'].count().reset_index(name='TotalLoans')
avg_loans_by_client = df_merged.groupby('RecipientType')['LoanID'].nunique().reset_index(name='UniqueLoanCount')

# Fusionar ambas para análisis combinado
summary = pd.merge(total_loans_by_client, avg_loans_by_client, on='RecipientType')
summary['AvgLoansPerClient'] = summary['TotalLoans'] / summary['UniqueLoanCount']

print("📋 Tabla resumen - Total y promedio de préstamos por tipo de cliente:")
st.dataframe(summary)

# Gráfico total de préstamos
plt.figure(figsize=(8, 4))
sns.barplot(data=summary, x='RecipientType', y='TotalLoans', palette='Set2')
plt.title("Total de Préstamos por Tipo de Cliente")
plt.ylabel("Número de Préstamos")
plt.xlabel("Tipo de Cliente")
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Agrupar total y únicos por tipo de cliente
total_loans_by_client = df.groupby('RecipientType')['LoanID'].count().reset_index(name='TotalLoans')
avg_loans_by_client = df.groupby('RecipientType')['LoanID'].nunique().reset_index(name='UniqueLoanCount')

# Calcular promedio por cliente
summary = pd.merge(total_loans_by_client, avg_loans_by_client, on='RecipientType')
summary['AvgLoansPerClient'] = summary['TotalLoans'] / summary['UniqueLoanCount']

# Fuente para Bokeh
source = ColumnDataSource(summary)

# Crear figura
p = figure(x_range=summary['RecipientType'].astype(str).tolist(),
           title="Total de Préstamos por Tipo de Cliente",
           x_axis_label="Tipo de Cliente",
           y_axis_label="Número de Préstamos",
           width=800, height=400,
           toolbar_location=None,
           background_fill_color=None,
           border_fill_color=None)

# Barras doradas estilo Stratify
p.vbar(x='RecipientType', top='TotalLoans', width=0.6, source=source,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Estética de texto y ejes
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Grid y rango
p.xgrid.grid_line_color = None
p.ygrid.grid_line_dash = 'dashed'
p.ygrid.grid_line_color = "#cccccc"
p.y_range.start = 0
p.outline_line_color = None

# Mostrar
show(p)

# Agrupar cantidad de préstamos por tipo de cliente
total_loans_by_client_type = df_merged.groupby('RecipientType').size().reset_index(name='TotalLoans')

# Calcular el total general de préstamos
grand_total_loans = total_loans_by_client_type['TotalLoans'].sum()

# Calcular el porcentaje de préstamos por tipo de cliente
total_loans_by_client_type['Percentage'] = (total_loans_by_client_type['TotalLoans'] / grand_total_loans) * 100

print("📋 Tabla resumen - Proporción de préstamos por tipo de cliente:")
st.dataframe(total_loans_by_client_type)


# Gráfico de barras para la proporción total de préstamos
plt.figure(figsize=(8, 5))
sns.barplot(data=total_loans_by_client_type, x='RecipientType', y='Percentage', palette='viridis')
plt.title("Distribución Porcentual del Total de Préstamos por Tipo de Cliente")
plt.ylabel("Porcentaje del Total de Préstamos (%)")
plt.xlabel("Tipo de Cliente")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Agrupar y calcular porcentaje
total_loans_by_client_type = df.groupby('RecipientType').size().reset_index(name='TotalLoans')
grand_total_loans = total_loans_by_client_type['TotalLoans'].sum()
total_loans_by_client_type['Percentage'] = (total_loans_by_client_type['TotalLoans'] / grand_total_loans) * 100

# Datos para Bokeh
recipient_types = total_loans_by_client_type['RecipientType'].astype(str).tolist()
percentages = total_loans_by_client_type['Percentage'].tolist()

# Crear figura
p = figure(x_range=recipient_types,
           title="Distribución Porcentual del Total de Préstamos por Tipo de Cliente",
           x_axis_label="Tipo de Cliente",
           y_axis_label="Porcentaje del Total de Préstamos (%)",
           width=800, height=400,
           toolbar_location=None,
           background_fill_color=None,
           border_fill_color=None)

# Dibujar barras doradas
p.vbar(x=recipient_types, top=percentages, width=0.6,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Estética de texto y ejes
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Líneas y bordes
p.xgrid.grid_line_color = None
p.ygrid.grid_line_dash = 'dashed'
p.ygrid.grid_line_color = "#cccccc"
p.y_range.start = 0
p.outline_line_color = None

# Mostrar
show(p)

# Agrupar por entidad (ID Aliada) y contar los préstamos
loans_per_entity = df_merged.groupby("ID Aliada")["LoanID"].count().reset_index(name="LoanCount")

# Identificar y quitar outliers (> 8 préstamos)
loans_per_entity_filtered_8 = loans_per_entity[loans_per_entity['LoanCount'] <= 8]

# Calcular el promedio de préstamos después de quitar outliers
average_loans_per_entity_filtered_8 = loans_per_entity_filtered_8['LoanCount'].mean()

print(f"📋 Promedio de préstamos por entidad (excluyendo > 8): {average_loans_per_entity_filtered_8:.2f}")

# Mostrar la tabla resumen con el conteo por entidad después de filtrar (> 8)
print("\n📋 Conteo de préstamos por entidad (después de quitar > 8):")
st.dataframe(loans_per_entity_filtered_8.head()) # Muestra las primeras filas

# Visualización del conteo de préstamos por entidad (histograma)

plt.figure(figsize=(10, 6))
sns.histplot(loans_per_entity_filtered_8['LoanCount'], bins=20, kde=True)
plt.title('Distribución del Conteo de Préstamos por Entidad (Excluyendo > 8 Préstamos)')
plt.xlabel('Número de Préstamos por Entidad')
plt.ylabel('Número de Entidades')
plt.grid(axis='y')
plt.show()

# Visualización del conteo de préstamos por entidad (boxplot)

plt.figure(figsize=(10, 6))
sns.boxplot(x=loans_per_entity_filtered_8['LoanCount'])
plt.title('Boxplot del Conteo de Préstamos por Entidad (Excluyendo > 8 Préstamos)')
plt.xlabel('Número de Préstamos por Entidad')
plt.show()

# Visualización adicional: Violin plot de los datos filtrados

plt.figure(figsize=(10, 6))
sns.violinplot(x=loans_per_entity_filtered_8['LoanCount'])
plt.title('Violin Plot del Conteo de Préstamos por Entidad (Excluyendo > 8 Préstamos)')
plt.xlabel('Número de Préstamos por Entidad')
plt.show()

output_notebook()

# Cargar y procesar datos
df = pd.read_csv("df_merged.csv")
loans_per_entity = df.groupby("ID Aliada")["LoanID"].count().reset_index(name="LoanCount")
loans_per_entity_filtered_8 = loans_per_entity[loans_per_entity['LoanCount'] <= 8]
data = loans_per_entity_filtered_8['LoanCount']

# Histograma
hist, edges = np.histogram(data, bins=20)

# KDE
kde = gaussian_kde(data)
x_kde = np.linspace(data.min(), data.max(), 200)
y_kde = kde(x_kde) * len(data) * (edges[1] - edges[0])

# ColumnDataSources
source_hist = ColumnDataSource(data=dict(top=hist, left=edges[:-1], right=edges[1:]))
source_kde = ColumnDataSource(data=dict(x=x_kde, y=y_kde))

# Crear figura con fondo transparente
p1 = figure(title="Distribución del Conteo de Préstamos por Entidad (Excluyendo > 8)",
            x_axis_label="Número de Préstamos por Entidad",
            y_axis_label="Número de Entidades",
            width=800, height=400,
            background_fill_color=None,
            border_fill_color=None)

# Barras doradas
p1.quad(top='top', bottom=0, left='left', right='right', source=source_hist,
        fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Línea KDE roja elegante
p1.line('x', 'y', source=source_kde, line_color="#e63946", line_width=3)

# HoverTool
p1.add_tools(HoverTool(tooltips=[
    ("Intervalo", "@left - @right"),
    ("Cantidad", "@top")
]))

# Estética de texto
p1.title.text_font = "helvetica"
p1.title.text_font_size = "14pt"
p1.title.text_color = "#1c1c1c"
p1.xaxis.axis_label_text_font = "helvetica"
p1.xaxis.axis_label_text_color = "#1c1c1c"
p1.xaxis.major_label_text_font = "helvetica"
p1.xaxis.major_label_text_color = "#333333"
p1.yaxis.axis_label_text_font = "helvetica"
p1.yaxis.axis_label_text_color = "#1c1c1c"
p1.yaxis.major_label_text_font = "helvetica"
p1.yaxis.major_label_text_color = "#333333"

# Líneas y grid
p1.grid.grid_line_alpha = 0.3
p1.grid.grid_line_color = "#cccccc"
p1.outline_line_color = None

# Mostrar
show(p1)

output_notebook()

# Calcular estadísticas
q1 = data.quantile(0.25)
q2 = data.quantile(0.5)
q3 = data.quantile(0.75)
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr
outliers = data[(data < lower) | (data > upper)]

# Crear figura con fondo transparente
p2 = figure(title="Boxplot del Conteo de Préstamos por Entidad (Excluyendo > 8)",
            x_axis_label="Número de Préstamos por Entidad",
            y_range=["Préstamos por Entidad"],
            width=800, height=300,
            background_fill_color=None,
            border_fill_color=None)

# Caja dorada
p2.hbar(y=["Préstamos por Entidad"], height=0.4, left=q1, right=q3,
        fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Mediana en rojo elegante
p2.segment(x0=q2, y0=0.8, x1=q2, y1=1.2, line_color="#e63946", line_width=2)

# Bigotes
p2.segment(x0=lower, y0=1, x1=q1, y1=1, line_color="#aaaaaa")
p2.segment(x0=q3, y0=1, x1=upper, y1=1, line_color="#aaaaaa")

# Outliers con puntos negros visibles
p2.scatter(x=outliers, y=["Préstamos por Entidad"] * len(outliers), size=7,
           marker="circle", fill_color="black", line_color=None, fill_alpha=0.8)

# Estilo de texto
p2.title.text_font = "helvetica"
p2.title.text_font_size = "14pt"
p2.title.text_color = "#1c1c1c"
p2.xaxis.axis_label_text_font = "helvetica"
p2.xaxis.axis_label_text_color = "#1c1c1c"
p2.xaxis.major_label_text_font = "helvetica"
p2.xaxis.major_label_text_color = "#333333"
p2.yaxis.major_label_text_font = "helvetica"
p2.yaxis.major_label_text_color = "#333333"

# Grid sobrio
p2.grid.grid_line_color = "#cccccc"
p2.grid.grid_line_alpha = 0.3
p2.outline_line_color = None

# Mostrar
show(p2)

output_notebook()

# KDE y violín simulado
kde_v = gaussian_kde(data)
x_v = np.linspace(data.min(), data.max(), 200)
y_v = kde_v(x_v)
y_scaled = y_v / y_v.max() * 0.4

x_violin = list(x_v) + list(x_v[::-1])
y_violin = list(y_scaled) + list(-y_scaled[::-1])

source_violin = ColumnDataSource(data=dict(x=x_violin, y=y_violin))

# Crear figura con fondo transparente
p3 = figure(title="Violin Plot del Conteo de Préstamos por Entidad (Excluyendo > 8)",
            width=800, height=400,
            x_axis_label="Número de Préstamos por Entidad",
            y_axis_label="Densidad (simulada)",
            background_fill_color=None,
            border_fill_color=None,
            tools="")

# Violín dorado claro
p3.patch('x', 'y', source=source_violin,
         fill_color="#cba660", fill_alpha=0.9, line_color="#cba660")

# Mediana en rojo elegante
median_val = data.median()
p3.line([median_val, median_val], [-0.45, 0.45], color="#e63946", line_width=3)

# Ajustes del eje y
p3.y_range.start = -0.5
p3.y_range.end = 0.5
p3.yaxis.visible = False

# Estética del texto
p3.title.text_font = "helvetica"
p3.title.text_font_size = "14pt"
p3.title.text_color = "#1c1c1c"
p3.xaxis.axis_label_text_font = "helvetica"
p3.xaxis.axis_label_text_color = "#1c1c1c"
p3.xaxis.major_label_text_font = "helvetica"
p3.xaxis.major_label_text_color = "#333333"

p3.grid.grid_line_color = "#cccccc"
p3.grid.grid_line_alpha = 0.3
p3.outline_line_color = None

# Mostrar
show(p3)

# Agrupar y calcular monto promedio por tipo de préstamo y tipo de cliente
avg_amount = df_merged.groupby(['RecipientType', 'LoanType'])['LoanAmount'].mean().reset_index()
avg_amount.rename(columns={'LoanAmount': 'AvgLoanAmount'}, inplace=True)

# Tabla resumen
pivot_amount = avg_amount.pivot(index='RecipientType', columns='LoanType', values='AvgLoanAmount')
print(" Tabla resumen - Monto promedio por tipo de préstamo y tipo de cliente:")
st.dataframe(pivot_amount)


# Gráfico
plt.figure(figsize=(8, 5))
sns.barplot(data=avg_amount, x='RecipientType', y='AvgLoanAmount', hue='LoanType')
plt.title("Monto Promedio del Préstamo por Cliente y Tipo de Préstamo")
plt.ylabel("Monto Promedio ($)")
plt.xlabel("Tipo de Cliente")
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Calcular monto promedio
avg_amount = df.groupby(['RecipientType', 'LoanType'])['LoanAmount'].mean().reset_index()
avg_amount.rename(columns={'LoanAmount': 'AvgLoanAmount'}, inplace=True)

# Asegurar strings
avg_amount['RecipientType'] = avg_amount['RecipientType'].astype(str)
avg_amount['LoanType'] = avg_amount['LoanType'].astype(str)

# Eje X como pares (RecipientType, LoanType)
recipient_types = sorted(avg_amount['RecipientType'].unique().tolist())
loan_types = sorted(avg_amount['LoanType'].unique().tolist())
x = [(rec, loan) for rec in recipient_types for loan in loan_types]

# Extraer montos
amounts = []
for rec in recipient_types:
    for loan in loan_types:
        value = avg_amount.loc[(avg_amount['RecipientType'] == rec) & (avg_amount['LoanType'] == loan), 'AvgLoanAmount']
        amounts.append(value.values[0] if not value.empty else 0)

# Fuente de datos
source = ColumnDataSource(data=dict(x=x, amounts=amounts))

# Crear figura con fondo transparente
p = figure(x_range=FactorRange(*x), height=400, width=800,
           title="Monto Promedio del Préstamo por Cliente y Tipo de Préstamo",
           y_axis_label="Monto Promedio ($)",
           x_axis_label="Tipo de Cliente",
           toolbar_location=None,
           background_fill_color=None,
           border_fill_color=None)

# Barras con paleta sobria (dorada y tonos cálidos)
stratify_palette = ["#cba660", "#000000", "#4a4a4a"]
p.vbar(x='x', top='amounts', width=0.8, source=source,
       fill_color=factor_cmap('x', palette=stratify_palette,
                              factors=loan_types, start=1))

# Estética del eje y texto
p.x_range.range_padding = 0.1
p.xaxis.major_label_orientation = 1
p.y_range.start = 0
p.ygrid.grid_line_dash = 'dashed'
p.xgrid.grid_line_color = None

p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Mostrar gráfico
show(p)

# Conteo por canal
channel_counts = df_merged['canal_ajustado'].value_counts().reset_index()
channel_counts.columns = ['Canal', 'Cantidad']

# Bar chart - participación por canal
plt.figure(figsize=(8, 5))
sns.barplot(x='Canal', y='Cantidad', data=channel_counts, palette='viridis')
plt.title("Participación de Préstamos por Canal")
plt.xlabel("Canal")
plt.ylabel("Número de Préstamos")
plt.xticks(rotation=45, ha='right') # Rotate labels for better readability if needed
plt.tight_layout()
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Conteo por canal
channel_counts = df['canal_ajustado'].value_counts().reset_index()
channel_counts.columns = ['Canal', 'Cantidad']

# Convertir a strings
canales = channel_counts['Canal'].astype(str).tolist()
cantidades = channel_counts['Cantidad'].tolist()

# Crear figura con fondo transparente
p = figure(x_range=canales,
           title="Participación de Préstamos por Canal",
           x_axis_label="Canal",
           y_axis_label="Número de Préstamos",
           width=800, height=400,
           toolbar_location=None,
           background_fill_color=None,
           border_fill_color=None)

# Barras doradas Stratify
p.vbar(x=canales, top=cantidades, width=0.6,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Estética de ejes y texto
p.xaxis.major_label_orientation = 0.8
p.y_range.start = 0
p.ygrid.grid_line_dash = 'dashed'
p.ygrid.grid_line_color = "#cccccc"
p.xgrid.grid_line_color = None
p.outline_line_color = None

p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Mostrar
show(p)

# Conteo por región
region_counts = df_merged['customer_region'].value_counts().reset_index()
region_counts.columns = ['Región', 'Cantidad']

# Pie chart - participación por región
plt.figure(figsize=(6, 6))
colors = sns.color_palette("Set2")
plt.pie(region_counts['Cantidad'], labels=region_counts['Región'], autopct='%1.1f%%', startangle=90, colors=colors)
plt.title("Participación de Préstamos por Región")
plt.axis('equal')
plt.tight_layout()
plt.show()

from bokeh.io import output_notebook, show
from bokeh.plotting import figure
import pandas as pd

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Conteo por región
region_counts = df['customer_region'].value_counts().reset_index()
region_counts.columns = ['Región', 'Cantidad']
region_counts = region_counts.sort_values('Cantidad')

# Listas
regiones = region_counts['Región'].astype(str).tolist()
cantidades = region_counts['Cantidad'].tolist()

# Crear figura con fondo transparente
p = figure(y_range=regiones,
           title="Participación de Préstamos por Región",
           x_axis_label="Número de Préstamos",
           width=800, height=400,
           toolbar_location=None,
           background_fill_color=None,
           border_fill_color=None)

# Dibujar barras doradas Stratify
p.hbar(y=regiones, right=cantidades, height=0.6,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Estética de texto y ejes
p.x_range.start = 0
p.ygrid.grid_line_color = None
p.xgrid.grid_line_dash = 'dashed'
p.xgrid.grid_line_color = "#cccccc"
p.outline_line_color = None

p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Mostrar gráfico
show(p)