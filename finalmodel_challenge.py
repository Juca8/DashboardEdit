# -*- coding: utf-8 -*-
"""FinalModel_Challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HZCD868Ez4GgyLEPHPn1V__ThNw6oj9R
"""

"""# **Stratify**
Main objective	Understand consumption behavior of allies

What we want to understand?
- Basic demand (e.,g, type of products, type, recypient)
- Demand Behaviour (e.g., frequency, time between loans, loans simultaneously)
- Communication strategy (e.g., when to reach out, potential risk flags)
- Split between cliente final and mercancia
"""

import warnings

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from math import pi
import unicodedata

from sklearn.utils import resample

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score
from sklearn.impute import SimpleImputer

from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

import joblib
import streamlit as st

import os

file_path = "Aliadas_team2.csv"

if os.path.exists(file_path):
    df_aliadas_team2 = pd.read_csv(file_path, encoding="latin1")
else:
    st.error(f"File not found: {file_path}")
    
df_aliadas_team2

df_aliadas_team2.columns, df_aliadas_team2.shape

df_aliadas_team2.dtypes

df_prestamos_team2 = pd.read_csv("/content/Prestamos_team2.csv",encoding = "latin1")

df_prestamos_team2.head()

df_prestamos_team2.columns, df_prestamos_team2.shape

df_prestamos_team2.isna().sum(), df_prestamos_team2.shape

df_merged = pd.merge(df_aliadas_team2, df_prestamos_team2, left_on="ID Aliada", right_on="Ally ID", how="left")
df_merged.head()

df_merged.columns, df_merged.shape

df_merged.isna().sum(),df_merged.shape

numeric_cols = df_merged.select_dtypes(include=[np.number]).columns
cols_with_nan = [col for col in numeric_cols if df_merged[col].isna().any()]

for col in cols_with_nan:
    na_indices = df_merged[df_merged[col].isna()].index
    valores_existentes = df_merged[col].dropna()

    mean = valores_existentes.mean()
    std = valores_existentes.std()

    for idx in na_indices:
        simulaciones = np.random.normal(loc=mean, scale=std, size=1000)
        simulaciones = np.clip(simulaciones, 0, np.inf)
        valor_imputado = np.mean(simulaciones)
        df_merged.at[idx, col] = valor_imputado

df_merged['Total_Llamadas'] = (
    df_merged['Llamadas_Colgadas'].fillna(0) +
    df_merged['Llamada_Contestada'].fillna(0)
)

df_merged.isna().sum(),df_merged.shape

# Step 0: Numeric conversion (ensure numeric columns are properly formatted)
numeric_columns = [
    'cuotas_pagadas', 'prestamos_outstanding',
    'cuotas_mora', 'cuotas_tarde', 'cuotas_pendientes',
    'dias_promedio', 'max_dias_mora'
]
for col in numeric_columns:
    df_merged[col] = pd.to_numeric(df_merged[col], errors='coerce')

# Step 1: Credit Status
def classify_credit_status(row):
    if row['cuotas_pagadas'] == 0 and row['prestamos_outstanding'] > 0:
        return "Defaulted"
    elif row['cuotas_mora'] >= 3 or row['max_dias_mora'] > 30:
        return "Delinquent"
    elif row['cuotas_tarde'] >= 1 or (0 < row['max_dias_mora'] <= 30):
        return "Late Payments"
    elif row['cuotas_pendientes'] == 0 and row['cuotas_mora'] == 0:
        return "Up to date"
    else:
        return "Unknown"

df_merged['credit_status'] = df_merged.apply(classify_credit_status, axis=1)

# Step 2: Contactability Level
def calculate_contactability(row):
    answered = row['Llamada_Contestada'] if pd.notna(row['Llamada_Contestada']) else 0
    if answered >= 3:
        return "High"
    elif answered in [1, 2]:
        return "Medium"
    else:
        return "Low"

df_merged['contactability_level'] = df_merged.apply(calculate_contactability, axis=1)

# Step 3: Commercial Risk
def calculate_commercial_risk(row):
    riesgo_fraude = bool(row['Fraude_Riesgo']) if pd.notna(row['Fraude_Riesgo']) else False
    llamadas_muchas = bool(row['>10_Llamadas']) if pd.notna(row['>10_Llamadas']) else False
    promesa_pago = bool(row['Promesa_de_Pago']) if pd.notna(row['Promesa_de_Pago']) else False

    if row['prestamo_mora'] > 0 and riesgo_fraude:
        return "Very High"
    elif row['cuotas_mora'] > 0 and llamadas_muchas:
        return "High"
    elif row['cuotas_tarde'] > 0 and promesa_pago:
        return "Medium"
    elif row['cuotas_pagadas'] > 0 and row['cuotas_mora'] == 0:
        return "Low"
    else:
        return "Unknown"

df_merged['commercial_risk'] = df_merged.apply(calculate_commercial_risk, axis=1)

# Step 4: Payment Frequency
def calculate_payment_frequency(row):
    hizo_mas_pagos = bool(row['Hizo_>1_Pago']) if pd.notna(row['Hizo_>1_Pago']) else False
    if hizo_mas_pagos and row['dias_promedio'] <= 30:
        return "Frequent"
    elif not hizo_mas_pagos and row['dias_promedio'] > 30:
        return "Sporadic"
    elif row['cuotas_pagadas'] == 0:
        return "No Payments"
    else:
        return "Other"

df_merged['payment_frequency'] = df_merged.apply(calculate_payment_frequency, axis=1)

# Step 5: Customer Region ‚Äî CORREGIDO
def normalize(text):
    if pd.isna(text):
        return ""
    text = text.upper()
    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')

region_north  = [normalize(s) for s in ["Nuevo Le√≥n", "Chihuahua", "Coahuila"]]
region_center = [normalize(s) for s in ["CDMX", "Estado de M√©xico", "Quer√©taro", "Hidalgo", "Puebla"]]
region_south  = [normalize(s) for s in ["Oaxaca", "Chiapas", "Guerrero", "Tabasco", "Veracruz"]]

def assign_region(state):
    state_norm = normalize(state)
    if state_norm in region_north:
        return "North"
    elif state_norm in region_center:
        return "Center"
    elif state_norm in region_south:
        return "South"
    else:
        return "Other"

df_merged['customer_region'] = df_merged['State'].apply(assign_region)

# Step 6: Collection Intensity
def calculate_collection_intensity(total_calls):
    total = total_calls if pd.notna(total_calls) else 0
    if total > 15:
        return "Very High"
    elif 6 <= total <= 15:
        return "Moderate"
    else:
        return "Low"

df_merged['collection_intensity'] = df_merged['Total_Llamadas'].apply(calculate_collection_intensity)

# Step 7: Effective Payer
def is_effective_payer(row):
    promesa       = bool(row['Promesa_de_Pago']) if pd.notna(row['Promesa_de_Pago']) else False
    pago_realizado = bool(row['Ya Pagaron'])     if pd.notna(row['Ya Pagaron'])     else False
    return "Yes" if promesa and pago_realizado else "No"

df_merged['effective_payer'] = df_merged.apply(is_effective_payer, axis=1)

# Step 8: Target Variable ‚Äì Intensive Use (assigned correctly)
def classify_intensive_use(row):
    return int(
        (row.get('prestamos_totales', 0) >= 4) or
        (row.get('prestamos_outstanding', 0) >= 2) or
        (row.get('dias_promedio', np.inf) <= 15)
    )

df_merged['intensive_use'] = df_merged.apply(classify_intensive_use, axis=1)

df_merged.isna().sum(),df_merged.shape

df_merged["Fraude_Riesgo"] = df_merged["Fraude_Riesgo"].fillna("No_fraude")

df_merged["Fraude_Riesgo"] = df_merged["Fraude_Riesgo"].apply(
    lambda x: 1 if str(x).strip().lower() == "fraude_riesgo" else 0
).astype("int")

df_merged["Fraude_Riesgo"].value_counts()

df_aliadas_team2["ultimo_pago"] = pd.to_datetime(df_aliadas_team2["ultimo_pago"], errors="coerce").dt.strftime('%d/%m/%Y %H:%M:%S')
#df_aliadas_team2["Cosecha"] = pd.to_datetime(df_aliadas_team2["ultimo_pago"], errors="coerce").dt.strftime('%d/%m/%Y %H:%M:%S')

df_merged['IssueDate'] = pd.to_datetime(df_merged['IssueDate'], format='%d/%m/%Y %H:%M:%S', errors='coerce')
df_merged['IssueMonth'] = df_merged['IssueDate'].dt.month
df_merged['IssueDay'] = df_merged['IssueDate'].dt.day
df_merged['IssueWeekday'] = df_merged['IssueDate'].dt.weekday
df_merged['IssueYear'] = df_merged['IssueDate'].dt.year

df_merged['IssueMonth'] = df_merged['IssueMonth'].astype('int64')
df_merged['IssueDay'] = df_merged['IssueDay'].astype('int64')
df_merged['IssueWeekday'] = df_merged['IssueWeekday'].astype('int64')
df_merged['IssueYear'] = df_merged['IssueYear'].astype('int64')

df_merged = df_merged.dropna(subset=['DisbursementMeans', 'cuotas_tarde'])

cols_to_drop = ['State','City', 'Ally_ID_x','']

df_merged = df_merged.drop(columns=[col for col in cols_to_drop if col in df_merged.columns])

numeric_cols = df_merged.select_dtypes(include=['float64', 'int64']).columns.tolist()

numeric_cols_to_int = [col for col in numeric_cols if col != 'dias_promedio']
for col in numeric_cols_to_int:
    df_merged[col] = df_merged[col].astype(int)

df_merged.dtypes

df_merged.isna().sum(),df_merged.shape

df_merged.head()

# Step 1: Feature selection for clustering
clustering_features = [
    'cuotas_pagadas', 'cuotas_tarde', 'cuotas_mora',
    'dias_promedio', 'Total_Llamadas'
]

df_clustering = df_merged[clustering_features].dropna()

# Step 2: Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_clustering)

# Step 3: Elbow Method
inertia = []
for k in range(1, 11):
    km = KMeans(n_clusters=k, init='k-means++', random_state=42)
    km.fit(X_scaled)
    inertia.append(km.inertia_)

plt.figure()
plt.plot(range(1, 11), inertia, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.grid()
plt.show()

# Step 4: Apply KMeans (based on elbow result, here k=3)
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)
df_merged['cluster'] = kmeans.fit_predict(X_scaled)

# Step 5: PCA for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df_merged['cluster'], cmap='viridis', alpha=0.6)
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('Allies Cluster Visualization (PCA)')
plt.colorbar(label='Cluster')
plt.grid()
plt.show()

# Cluster analysis: summarize key variables per cluster
cluster_summary = df_merged.groupby('cluster')[
    ['cuotas_pagadas', 'cuotas_tarde', 'cuotas_mora', 'dias_promedio', 'Total_Llamadas']
].mean().round(2)

# Add contextual info: region distribution per cluster
region_distribution = df_merged.groupby('cluster')['customer_region'].value_counts(normalize=True).unstack().fillna(0).round(2)

# Combine both into one view
summary_with_region = cluster_summary.copy()
for region in region_distribution.columns:
    summary_with_region[f'Region_{region}_%'] = region_distribution[region] * 100

summary_with_region

# Step 1: Define input (X) and target (y)
leakage_cols = [
    'Aliada_ID', 'ID Aliada', 'Ally_ID', 'Ally_ID_y',
    'LoanID', 'ultimo_pago', 'effective_payer',
    'Promesa_de_Pago', 'Ya Pagaron'
]

X = df_merged.drop(columns=leakage_cols + ['intensive_use'], errors='ignore')
y = df_merged['intensive_use']

# Step 2: Identify column types
num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_cols = X.select_dtypes(include='object').columns.tolist()

# Step 3: Define models
cv_models = {
    'DecisionTree': DecisionTreeClassifier(random_state=42),
    'SVC': SVC(kernel='linear', random_state=42),
    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
    'RandomForest': RandomForestClassifier(random_state=42),
    'MLPClassifier': MLPClassifier(max_iter=500, random_state=42)
}

# Step 4: Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_results = []

for name, model in cv_models.items():
    train_scores = []
    test_scores = []

    for train_idx, test_idx in cv.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        pipeline = Pipeline(steps=[
            ('preprocessor', ColumnTransformer(transformers=[
                ('num', StandardScaler(), num_cols),
                ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
            ])),
            ('classifier', model)
        ])

        pipeline.fit(X_train, y_train)

        y_train_pred = pipeline.predict(X_train)
        y_test_pred = pipeline.predict(X_test)

        train_scores.append(balanced_accuracy_score(y_train, y_train_pred))
        test_scores.append(balanced_accuracy_score(y_test, y_test_pred))

    cv_results.append({
        'Model': name,
        'Train Mean': np.mean(train_scores),
        'Train Std': np.std(train_scores),
        'Test Mean': np.mean(test_scores),
        'Test Std': np.std(test_scores)
    })

cv_df = pd.DataFrame(cv_results).sort_values(by='Test Mean', ascending=False)
st.dataframe(cv_df)

# Define metrics to plot
metrics = ['balanced accuracy'] # You can add other metrics here if needed, e.g., ['balanced accuracy', 'accuracy', 'precision', 'recall', 'f1']

# Get the names of the models from the cv_models dictionary
model_names = list(cv_models.keys())

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))
axes = axes.flatten()

# Assuming you want to plot up to 4 metrics if defined in the 'metrics' list
for i, metric in enumerate(metrics[:len(axes)]): # Iterate only up to the number of available subplots
    metric_data = []

    # Define the scoring function for the current metric
    if metric == 'balanced accuracy':
        scorer = make_scorer(balanced_accuracy_score)
    elif metric == 'accuracy':
         scorer = make_scorer(accuracy_score)
    elif metric == 'precision':
         scorer = make_scorer(precision_score)
    elif metric == 'recall':
         scorer = make_scorer(recall_score)
    elif metric == 'f1':
         scorer = make_scorer(f1_score)
    else:
        print(f"Warning: Unknown metric '{metric}'. Skipping.")
        continue # Skip to the next metric if unknown

    for model_name in model_names:
        pipeline = Pipeline(steps=[
            ('preprocessor', ColumnTransformer(transformers=[
                ('num', StandardScaler(), num_cols),
                ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
            ])),
            ('classifier', cv_models[model_name])
        ])

        # Use the defined scorer for cross_val_score
        scores = cross_val_score(pipeline, X, y, cv=cv, scoring=scorer)
        metric_data.append(scores)

    # Plot the boxplot on the current axis
    sns.boxplot(data=metric_data, ax=axes[i])
    axes[i].set_title(f'{metric.replace("_", " ").title()} by Model') # Improve title formatting
    axes[i].set_xticks(range(len(model_names)))
    axes[i].set_xticklabels(model_names)
    axes[i].set_ylabel(metric.replace("_", " ").title())

# Turn off any unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])


plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
sns.countplot(x='intensive_use', data=df_merged)
plt.title('Class Balance of Intensive Use')
plt.xlabel('Intensive Use (0: No, 1: Yes)')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['No', 'Yes'])
plt.show()

# Step 1: Reuse preprocessor
preprocessor = ColumnTransformer(transformers=[
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
])

# Step 2: Apply preprocessing
X_processed = preprocessor.fit_transform(X)

# Step 3: Apply SMOTE
smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X_processed, y)

# Step 4: Train-test split AFTER SMOTE (balanced data)
X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(
    X_smote, y_smote, test_size=0.2, stratify=y_smote, random_state=42
)

# Step 5: Confirm shapes and balance
print("After SMOTE:")
print("Training set shape:", X_train_smote.shape, y_train_smote.shape)
print("Test set shape:", X_test_smote.shape, y_test_smote.shape)
print("Class distribution (train):", pd.Series(y_train_smote).value_counts())
print("Class distribution (test):", pd.Series(y_test_smote).value_counts())

# Suprimir warnings de OneHotEncoder por categor√≠as nuevas
warnings.filterwarnings("ignore", message="Found unknown categories*")

# Asegurar que y es Series
y = pd.Series(y)

# Definir CV
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Guardar resultados
cv_results_smote_detailed = []

for name, model in cv_models.items():
    train_scores = []
    test_scores = []

    for train_idx, test_idx in cv.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        pipeline = ImbPipeline(steps=[
            ('preprocessor', ColumnTransformer(transformers=[
                ('num', StandardScaler(), num_cols),
                ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
            ])),
            ('smote', SMOTE(random_state=42)),
            ('classifier', model)
        ])

        pipeline.fit(X_train, y_train)

        y_train_pred = pipeline.predict(X_train)
        y_test_pred = pipeline.predict(X_test)

        train_scores.append(balanced_accuracy_score(y_train, y_train_pred))
        test_scores.append(balanced_accuracy_score(y_test, y_test_pred))

    cv_results_smote_detailed.append({
        'Model': name,
        'Train Mean': np.mean(train_scores),
        'Train Std': np.std(train_scores),
        'Test Mean': np.mean(test_scores),
        'Test Std': np.std(test_scores)
    })

# Mostrar resultados ordenados por rendimiento en test
cv_df_smote_detailed = pd.DataFrame(cv_results_smote_detailed).sort_values(by='Test Mean', ascending=False)
st.dataframe(cv_df_smote_detailed)

# Define metrics
metrics = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score)
}

# Plot setup
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))
axes = axes.flatten()

# Boxplots for each metric
for i, (metric_name, scorer) in enumerate(metrics.items()):
    metric_data = []

    for model_name, model in cv_models.items():
        pipeline = ImbPipeline(steps=[
            ('preprocessor', ColumnTransformer(transformers=[
                ('num', StandardScaler(), num_cols),
                ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
            ])),
            ('smote', SMOTE(random_state=42)),
            ('classifier', model)
        ])

        scores = cross_val_score(pipeline, X, y, cv=cv, scoring=scorer)
        metric_data.append(scores)

    sns.boxplot(data=metric_data, ax=axes[i])
    axes[i].set_title(f'{metric_name.capitalize()} with SMOTE')
    axes[i].set_xticks(range(len(cv_models)))  # ‚úîÔ∏è Establece ticks
    axes[i].set_xticklabels(list(cv_models.keys()))  # ‚úîÔ∏è Ahora puedes usar los labels
    axes[i].set_ylabel(metric_name.capitalize())

plt.tight_layout()
plt.show()

splits = [0.2, 0.25, 0.3]
results = []

from imblearn.pipeline import Pipeline as ImbPipeline

results_smote = []  # Nueva lista de resultados con SMOTE

for test_size in splits:
    print(f"\n=================== Random Forest with SMOTE - Test Size {test_size} ===================")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, stratify=y, random_state=42
    )

    preprocessor = ColumnTransformer([
        ('num', StandardScaler(), num_cols),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
    ])

    pipeline_rf_smote = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', SMOTE(random_state=42)),
        ('classifier', RandomForestClassifier(random_state=42))
    ])

    param_grid_rf = {
        'classifier__n_estimators': [50, 100],
        'classifier__max_depth': [4, 5, 10],
        'classifier__min_samples_split': [2, 5],
        'classifier__min_samples_leaf': [5, 10]
    }

    grid_rf = GridSearchCV(
        pipeline_rf_smote,
        param_grid_rf,
        scoring='balanced_accuracy',
        cv=5,
        n_jobs=-1
    )
    grid_rf.fit(X_train, y_train)

    print(f"‚úÖ Best RandomForest Params (SMOTE): {grid_rf.best_params_}")

    y_proba_rf = grid_rf.predict_proba(X_test)[:, 1]
    auc_rf = roc_auc_score(y_test, y_proba_rf)
    print(f"AUC (RF): {auc_rf:.4f}")

    fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_proba_rf)
    optimal_idx_rf = (tpr_rf - fpr_rf).argmax()
    optimal_threshold_rf = thresholds_rf[optimal_idx_rf]
    y_pred_rf = (y_proba_rf >= optimal_threshold_rf).astype(int)

    plt.figure()
    plt.plot(fpr_rf, tpr_rf, label=f'AUC = {auc_rf:.4f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC Curve - Random Forest with SMOTE (Test size {test_size})")
    plt.legend()
    plt.grid()
    plt.show()

    print("üìä Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred_rf))
    print("\nüìã Classification Report:")
    print(classification_report(y_test, y_pred_rf))

    results_smote.append({
        'Model': 'RandomForest + SMOTE',
        'Test Size': test_size,
        'AUC': auc_rf,
        'Accuracy': accuracy_score(y_test, y_pred_rf),
        'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred_rf),
        'Precision': precision_score(y_test, y_pred_rf),
        'Recall': recall_score(y_test, y_pred_rf),
        'F1-score': f1_score(y_test, y_pred_rf)
    })

for test_size in splits:
    print(f"\n=================== Logistic Regression with SMOTE - Test Size {test_size} ===================")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, stratify=y, random_state=42
    )

    preprocessor = ColumnTransformer([
        ('num', StandardScaler(), num_cols),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
    ])

    pipeline_lr_smote = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', SMOTE(random_state=42)),
        ('classifier', LogisticRegression(solver='liblinear', random_state=42))
    ])

    param_grid_lr = {
        'classifier__C': [0.01, 0.1, 1, 10],
        'classifier__penalty': ['l1', 'l2']
    }

    grid_lr = GridSearchCV(
        pipeline_lr_smote,
        param_grid_lr,
        scoring='balanced_accuracy',
        cv=5,
        n_jobs=-1
    )
    grid_lr.fit(X_train, y_train)

    print(f"‚úÖ Best LogisticRegression Params (SMOTE): {grid_lr.best_params_}")

    y_proba_lr = grid_lr.predict_proba(X_test)[:, 1]
    auc_lr = roc_auc_score(y_test, y_proba_lr)
    print(f"AUC (LR): {auc_lr:.4f}")

    fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_proba_lr)
    optimal_idx_lr = (tpr_lr - fpr_lr).argmax()
    optimal_threshold_lr = thresholds_lr[optimal_idx_lr]
    y_pred_lr = (y_proba_lr >= optimal_threshold_lr).astype(int)

    plt.figure()
    plt.plot(fpr_lr, tpr_lr, label=f'AUC = {auc_lr:.4f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC Curve - Logistic Regression with SMOTE (Test size {test_size})")
    plt.legend()
    plt.grid()
    plt.show()

    print("üìä Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred_lr))
    print("\nüìã Classification Report:")
    print(classification_report(y_test, y_pred_lr))

    results_smote.append({
        'Model': 'LogisticRegression + SMOTE',
        'Test Size': test_size,
        'AUC': auc_lr,
        'Accuracy': accuracy_score(y_test, y_pred_lr),
        'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred_lr),
        'Precision': precision_score(y_test, y_pred_lr),
        'Recall': recall_score(y_test, y_pred_lr),
        'F1-score': f1_score(y_test, y_pred_lr)
    })

# Mostrar resumen de ambos modelos
results_smote_df = pd.DataFrame(results_smote)
st.dataframe(results_smote_df)

# 1. Selecci√≥n de variables ampliadas
expanded_features = [
    # Demand Behaviour
    'dias_promedio', 'prestamos_outstanding', 'prestamo_mora',
    'cuotas_mora', 'cuotas_tarde', 'cuotas_pagadas',
    'max_dias_mora', 'dias_mora', 'Hizo_>1_Pago',
    'Total_Llamadas', '>10_Llamadas', 'TasaMora',

    # Basic Demand
    'prestamos_totales', 'LoanAmount', 'LoanType',
    'DisbursementMeans', 'RecipientType',

    # Contexto estrat√©gico
    'customer_region'
]

# 2. Definir X (features) e y (target)
X = df_merged[expanded_features]
y = df_merged['intensive_use']

# 3. Separar num√©ricas y categ√≥ricas
num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_cols = X.select_dtypes(include=['object']).columns.tolist()

# 4. Pipeline de preprocesamiento
preprocessor = ColumnTransformer(transformers=[
    ('num', Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ]), num_cols),
    ('cat', Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ]), cat_cols)
])

# 5. Modelo sin regularizaci√≥n
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000))
])

# 6. Entrenar modelo
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)
model.fit(X_train, y_train)

# 7. Obtener nombres de features despu√©s del OneHotEncoder
ohe = model.named_steps['preprocessor'].named_transformers_['cat']['onehot']
encoded_cat_features = ohe.get_feature_names_out(cat_cols)
all_features = np.concatenate([num_cols, encoded_cat_features])

# 8. Extraer coeficientes
coefficients = model.named_steps['classifier'].coef_[0]

# 9. Crear DataFrame de importancia
importances_df = pd.DataFrame({
    'Feature': all_features,
    'Coefficient': coefficients,
    'Abs_Coefficient': np.abs(coefficients)
}).sort_values(by='Abs_Coefficient', ascending=False)

# 10. Mostrar top 15
importances_df.reset_index(drop=True, inplace=True)
st.dataframe(importances_df.head(15))

# 1. Definir variables importantes
top_5_features = ['dias_promedio', 'RecipientType', 'LoanType', 'DisbursementMeans', 'customer_region']
num_cols = ['dias_promedio']
cat_cols = ['RecipientType', 'LoanType', 'DisbursementMeans', 'customer_region']

X_raw = df_merged[top_5_features]
y_raw = df_merged['intensive_use']

# 2. Preprocesamiento + SMOTE antes del split
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
])

X_processed = preprocessor.fit_transform(X_raw)

smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X_processed, y_raw)

# Confirmar balanceo
print("‚úÖ Distribuci√≥n despu√©s de SMOTE:")
print(pd.Series(y_smote).value_counts())

# 3. GridSearch + ROC por cada test size
splits = [0.2, 0.25, 0.3]
results_smote_presplit = []

for test_size in splits:
    print(f"\n================ Logistic Regression (SMOTE before split) - Test Size {test_size} =================")

    X_train, X_test, y_train, y_test = train_test_split(
        X_smote, y_smote, test_size=test_size, stratify=y_smote, random_state=42
    )

    model = LogisticRegression(solver='liblinear', random_state=42)

    param_grid = {
        'C': [0.01, 0.1, 1, 10],
        'penalty': ['l1', 'l2']
    }

    grid = GridSearchCV(model, param_grid, scoring='balanced_accuracy', cv=5, n_jobs=-1)
    grid.fit(X_train, y_train)

    print(f"‚úÖ Best parameters: {grid.best_params_}")

    y_proba = grid.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_proba)
    print(f"AUC: {auc:.4f}")

    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    optimal_idx = (tpr - fpr).argmax()
    optimal_threshold = thresholds[optimal_idx]
    y_pred = (y_proba >= optimal_threshold).astype(int)

    # ROC curve
    plt.figure()
    plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC - Logistic Regression (SMOTE before split) - Test size {test_size}")
    plt.legend()
    plt.grid()
    plt.show()

    print("üìä Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\nüìã Classification Report:")
    print(classification_report(y_test, y_pred))

    results_smote_presplit.append({
        'Model': 'LogReg + SMOTE before split',
        'Test Size': test_size,
        'AUC': auc,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-score': f1_score(y_test, y_pred)
    })

# Mostrar tabla resumen final
results_smote_presplit_df = pd.DataFrame(results_smote_presplit)
st.dataframe(results_smote_presplit_df)

# 1. Definir variables importantes
top_5_features = ['dias_promedio', 'RecipientType', 'LoanType', 'DisbursementMeans', 'customer_region']
num_cols = ['dias_promedio']
cat_cols = ['RecipientType', 'LoanType', 'DisbursementMeans', 'customer_region']

X_raw = df_merged[top_5_features]
y_raw = df_merged['intensive_use']

# 2. Preprocesamiento (antes del split)
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
])

X_processed = preprocessor.fit_transform(X_raw)

# 3. GridSearch + ROC para diferentes test sizes
splits = [0.2, 0.25, 0.3]
results_smote_postsplit = []

for test_size in splits:
    print(f"\n================ Logistic Regression (SMOTE after split) - Test Size {test_size} =================")

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y_raw, test_size=test_size, stratify=y_raw, random_state=42
    )

    # SMOTE SOLO en el entrenamiento
    smote = SMOTE(random_state=42)
    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

    # Modelo + GridSearch
    model = LogisticRegression(solver='liblinear', random_state=42)
    param_grid = {
        'C': [0.01, 0.1, 1, 10],
        'penalty': ['l1', 'l2']
    }
    grid = GridSearchCV(model, param_grid, scoring='balanced_accuracy', cv=5, n_jobs=-1)
    grid.fit(X_train_res, y_train_res)

    print(f"‚úÖ Best parameters: {grid.best_params_}")

    # Predicci√≥n
    y_proba = grid.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_proba)
    print(f"AUC: {auc:.4f}")

    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    optimal_idx = (tpr - fpr).argmax()
    optimal_threshold = thresholds[optimal_idx]
    y_pred = (y_proba >= optimal_threshold).astype(int)

    # ROC curve
    plt.figure()
    plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC - Logistic Regression (SMOTE after split) - Test size {test_size}")
    plt.legend()
    plt.grid()
    plt.show()

    print("üìä Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\nüìã Classification Report:")
    print(classification_report(y_test, y_pred))

    results_smote_postsplit.append({
        'Model': 'LogReg + SMOTE after split',
        'Test Size': test_size,
        'AUC': auc,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-score': f1_score(y_test, y_pred)
    })

# Mostrar resumen
results_smote_postsplit_df = pd.DataFrame(results_smote_postsplit)
st.dataframe(results_smote_postsplit_df)

# 2. Crear matriz de confusi√≥n
cm = confusion_matrix(y_test, y_pred)

# 3. Graficar
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['No Intensivo', 'Intensivo'],
            yticklabels=['No Intensivo', 'Intensivo'])
plt.xlabel('Predicci√≥n')
plt.ylabel('Valor Real')
plt.title('Matriz de Confusi√≥n - Logistic Regression (Test Size 0.30)')
plt.show()



"""## Graphic Insights"""

from bokeh.plotting import figure, show, output_notebook
from bokeh.models import ColumnDataSource, HoverTool, Legend, LinearColorMapper, ColorBar, BasicTicker, PrintfTickFormatter, FactorRange
from bokeh.palettes import Category10, RdBu, Category20, Viridis256, linear_palette
from bokeh.palettes import Set2_8
from bokeh.io import output_notebook
from bokeh.transform import transform, factor_cmap
from scipy.stats import gaussian_kde
from bokeh.models import LabelSet

from bokeh.io import output_notebook, show
from bokeh.plotting import figure
import pandas as pd


from scipy.stats import gaussian_kde

# Total de pr√©stamos por aliada
basic_demand = df_merged.groupby("ID Aliada")["LoanID"].count().reset_index()
basic_demand.columns = ["ID Aliada", "basic_demand"]

plt.figure(figsize=(10, 6))
sns.histplot(basic_demand['basic_demand'], bins=20, kde=True)
plt.title('Distribution of Basic Demand (Total Loans per Ally)')
plt.xlabel('Number of Loans')
plt.ylabel('Number of Allies')
plt.grid(axis='y')
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Calcular demanda b√°sica por Aliada
basic_demand = df.groupby("ID Aliada")["LoanID"].count().reset_index()
basic_demand.columns = ["ID Aliada", "basic_demand"]

# Histograma
hist, edges = np.histogram(basic_demand["basic_demand"], bins=20)

# Curva KDE
kde = gaussian_kde(basic_demand["basic_demand"])
x_kde = np.linspace(basic_demand["basic_demand"].min(), basic_demand["basic_demand"].max(), 200)
y_kde = kde(x_kde) * len(basic_demand) * (edges[1] - edges[0])

# Fuentes de datos para Bokeh
source_hist = ColumnDataSource(data=dict(top=hist, left=edges[:-1], right=edges[1:]))
source_kde = ColumnDataSource(data=dict(x=x_kde, y=y_kde))

# Crear figura con fondo transparente
p = figure(title="Distribuci√≥n de la Demanda B√°sica (Total de Pr√©stamos por Aliada)",
           x_axis_label="N√∫mero de Pr√©stamos",
           y_axis_label="N√∫mero de Aliadas",
           width=800, height=400,
           tools="pan,box_zoom,reset,save",
           background_fill_color=None,
           border_fill_color=None)

# Histograma en dorado
p.quad(top='top', bottom=0, left='left', right='right', source=source_hist,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9, legend_label="Histograma")

# KDE en rojo elegante
p.line('x', 'y', source=source_kde, line_color="#e63946", line_width=3, legend_label="Curva KDE")

# Hover estilizado
hover = HoverTool(tooltips=[
    ("Rango", "@left{0} - @right{0}"),
    ("Cantidad", "@top")
])
p.add_tools(hover)

# Tipograf√≠a elegante
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"

p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"

p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Leyenda elegante
p.legend.label_text_font = "helvetica"
p.legend.label_text_color = "#1c1c1c"
p.legend.background_fill_alpha = 0.0
p.legend.border_line_color = None
p.legend.location = "top_right"

# Grid sutil
p.grid.grid_line_color = "#cccccc"
p.grid.grid_line_alpha = 0.3
p.outline_line_color = None

# Mostrar gr√°fica
show(p)

plt.figure(figsize=(10, 6))
sns.boxplot(x=basic_demand['basic_demand'])
plt.title('Boxplot of Basic Demand (Total Loans per Ally)')
plt.xlabel('Number of Loans')
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Calcular demanda b√°sica por Aliada
basic_demand = df.groupby("ID Aliada")["LoanID"].count().reset_index()
basic_demand.columns = ["ID Aliada", "basic_demand"]
data = basic_demand["basic_demand"]

# Calcular cuartiles
q1 = data.quantile(0.25)
q2 = data.quantile(0.50)
q3 = data.quantile(0.75)
iqr = q3 - q1
upper = q3 + 1.5 * iqr
lower = q1 - 1.5 * iqr

# Outliers
outliers = data[(data < lower) | (data > upper)]

# Crear figura con fondo transparente
p = figure(title="Boxplot de la Demanda B√°sica (Pr√©stamos Totales por Aliada)",
           width=800, height=300, x_axis_label="N√∫mero de Pr√©stamos",
           y_range=["Basic Demand"],
           tools="pan,box_zoom,reset,save",
           background_fill_color=None,
           border_fill_color=None)

# Caja dorada
p.hbar(y=["Basic Demand"], height=0.4, left=q1, right=q3,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Mediana en rojo
p.segment(x0=q2, y0=0.8, x1=q2, y1=1.2, line_color="#e63946", line_width=2)

# Bigotes
p.segment(x0=lower, y0=1, x1=q1, y1=1, line_color="#aaaaaa")
p.segment(x0=q3, y0=1, x1=upper, y1=1, line_color="#aaaaaa")

# Outliers en negro m√°s visibles
p.scatter(x=outliers, y=["Basic Demand"] * len(outliers), size=8, marker="circle",
          fill_alpha=0.9, fill_color="#000000", line_color=None)

# Est√©tica premium Stratify
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Grid sutil
p.grid.grid_line_color = "#cccccc"
p.grid.grid_line_alpha = 0.3
p.outline_line_color = None

# Mostrar gr√°fico
show(p)

# 1. Extract Year and Month
df_merged["Year"] = df_merged["IssueDate"].dt.year
df_merged["Month"] = df_merged["IssueDate"].dt.strftime('%B') # Extract month name (e.g., January, February)

# 2. Group by Year and Month
seasonality_by_year = df_merged.groupby(["Year", "Month"])["LoanID"].count().reset_index()
seasonality_by_year.columns = ["Year", "Month", "MonthlyDemand"]

# Ensure consistent month ordering for each year
ordered_months = ["January", "February", "March", "April", "May", "June",
                  "July", "August", "September", "October", "November", "December"]
seasonality_by_year["Month"] = pd.Categorical(seasonality_by_year["Month"], categories=ordered_months, ordered=True)
seasonality_by_year = seasonality_by_year.sort_values(["Year", "Month"])

# 3. Filter for each year
seasonality_2024 = seasonality_by_year[seasonality_by_year["Year"] == 2024].reset_index(drop=True)
seasonality_2025 = seasonality_by_year[seasonality_by_year["Year"] == 2025].reset_index(drop=True)

# Filter the data to include only 'CONCORD' and 'Mercado Abierto'
concord_mercado_data = df_merged[df_merged['canal_ajustado'].isin(['CONCORD', 'Mercado Abierto'])].copy()

# Ensure 'IssueDate' is datetime and extract Year and Month
concord_mercado_data['IssueDate'] = pd.to_datetime(concord_mercado_data['IssueDate'], errors='coerce')
concord_mercado_data = concord_mercado_data.dropna(subset=['IssueDate']) # Drop rows where date conversion failed
concord_mercado_data["Year"] = concord_mercado_data["IssueDate"].dt.year
concord_mercado_data["Month"] = concord_mercado_data["IssueDate"].dt.strftime('%B')

# Group by Year, Month, and canal_ajustado to get loan counts
canal_monthly_demand = concord_mercado_data.groupby(["Year", "Month", "canal_ajustado"])["LoanID"].count().reset_index()
canal_monthly_demand.columns = ["Year", "Month", "Canal", "MonthlyDemand"]

# Ensure consistent month ordering for each year
ordered_months = ["January", "February", "March", "April", "May", "June",
                  "July", "August", "September", "October", "November", "December"]
canal_monthly_demand["Month"] = pd.Categorical(canal_monthly_demand["Month"], categories=ordered_months, ordered=True)
canal_monthly_demand = canal_monthly_demand.sort_values(["Year", "Month"])

# Filter for the desired period (March 2024 to February 2025)
canal_seasonality_plot_data = pd.concat([
    canal_monthly_demand[(canal_monthly_demand["Year"] == 2024) & (canal_monthly_demand["Month"].isin(ordered_months[2:]))], # Mar-Dec 2024
    canal_monthly_demand[(canal_monthly_demand["Year"] == 2025) & (canal_monthly_demand["Month"].isin(ordered_months[:2]))]  # Jan-Feb 2025
])

# Create a combined 'Year-Month' string for the x-axis
canal_seasonality_plot_data["Year-Month"] = canal_seasonality_plot_data["Year"].astype(str) + "-" + canal_seasonality_plot_data["Month"].astype(str)

# Define the desired order for plotting (Mar 2024 -> Feb 2025)
plot_order = []
for month in ordered_months[2:]:
    plot_order.append(f"2024-{month}")
for month in ordered_months[:2]:
    plot_order.append(f"2025-{month}")

# Ensure the 'Year-Month' column is ordered correctly for plotting
canal_seasonality_plot_data["Year-Month"] = pd.Categorical(canal_seasonality_plot_data["Year-Month"], categories=plot_order, ordered=True)
canal_seasonality_plot_data = canal_seasonality_plot_data.sort_values("Year-Month")

# Calculate the total monthly demand across the selected channels for the trendline
total_monthly_demand_concord_mercado = canal_seasonality_plot_data.groupby("Year-Month")["MonthlyDemand"].sum().reset_index()
total_monthly_demand_concord_mercado.columns = ["Year-Month", "TotalMonthlyDemand"]

# Create the combined plot
plt.figure(figsize=(14, 7))

# Bar plot for the breakdown by channel
bar_plot = sns.barplot(
    data=canal_seasonality_plot_data,
    x="Year-Month",
    y="MonthlyDemand",
    hue="Canal",
    dodge=False,  # Stack the bars
    palette='viridis'
)

# Add the line plot for the total monthly demand
line_plot = sns.lineplot(
    data=total_monthly_demand_concord_mercado,
    x="Year-Month",
    y="TotalMonthlyDemand",
    marker='o',
    color='red',  # Set the line color
    label='Total Monthly Demand' # Label for the legend
)

# Customize the plot
plt.title("Monthly Loan Demand by Channel (CONCORD, Mercado Abierto) with Total Trendline (March 2024 - February 2025)")
plt.xlabel("Month")
plt.ylabel("Number of Loans")
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', linestyle='--')
plt.legend(title="Channel", loc='upper left') # Add legend for both bars and line
plt.tight_layout()
plt.show()

output_notebook()

# Colores elegantes
channel_colors = {
    "CONCORD": "#cba660",
    "Mercado Abierto": "#4a4a4a"
}

canales = canal_seasonality_plot_data["Canal"].unique()

# Figura con mayor ancho y espacio en eje X
p = figure(x_range=plot_order, height=450, width=1100,
           title="Demanda Mensual de Pr√©stamos por Canal (CONCORD, Mercado Abierto) con L√≠nea de Tendencia",
           x_axis_label="Mes", y_axis_label="N√∫mero de Pr√©stamos",
           background_fill_color=None, border_fill_color=None,
           toolbar_location=None,
           min_border_left=50, min_border_right=50)

# Barras por canal
for canal in canales:
    canal_data = canal_seasonality_plot_data[canal_seasonality_plot_data["Canal"] == canal]
    source = ColumnDataSource(data=dict(
        x=canal_data["Year-Month"],
        y=canal_data["MonthlyDemand"]
    ))
    p.vbar(x='x', top='y', width=0.8, source=source,
           color=channel_colors.get(canal, "#888888"),
           legend_label=canal, muted_alpha=0.2)

# L√≠nea de tendencia total
source_line = ColumnDataSource(total_monthly_demand_concord_mercado)
p.line(x='Year-Month', y='TotalMonthlyDemand', source=source_line,
       line_color='#e63946', line_width=2, legend_label="Demanda Total")

# Puntos sobre la l√≠nea (compatible Bokeh 3.4+)
p.scatter(x='Year-Month', y='TotalMonthlyDemand', source=source_line,
          size=6, marker="circle", fill_color='#e63946',
          line_color=None, legend_label="Demanda Total")

# Rotar etiquetas del eje X
p.xaxis.major_label_orientation = pi / 3

# Est√©tica premium
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Grid sobrio
p.xgrid.grid_line_dash = 'dashed'
p.grid.grid_line_color = "#cccccc"
p.grid.grid_line_alpha = 0.3
p.outline_line_color = None

# Leyenda refinada
p.legend.location = "top_left"
p.legend.label_text_font = "helvetica"
p.legend.label_text_color = "#1c1c1c"
p.legend.background_fill_alpha = 0.0
p.legend.title = "Canal"
p.legend.click_policy = "mute"

# Mostrar
show(p)

df_merged["TasaMora"] = df_merged["cuotas_mora"] / df_merged["cuotas_pagadas"]
mora_por_aliada = df_merged.groupby("ID Aliada")["TasaMora"].mean().sort_values(ascending=False)

variables = ["cuotas_pagadas", "cuotas_mora", "cuotas_tarde", "dias_promedio", "max_dias_mora"]
sns.heatmap(df_merged[variables].corr(), annot=True, cmap="coolwarm")

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")
df["TasaMora"] = df["cuotas_mora"] / df["cuotas_pagadas"]

# Variables seleccionadas
variables = ["cuotas_pagadas", "cuotas_mora", "cuotas_tarde", "dias_promedio", "max_dias_mora"]
corr_matrix = df[variables].corr().round(2)
corr_long = corr_matrix.stack().reset_index()
corr_long.columns = ["var1", "var2", "correlation"]

source = ColumnDataSource(corr_long)

# Paleta Stratify personalizada (sin interpolaci√≥n)
stratify_palette = ["#e63946", "#f1c27d", "#cba660", "#4a4a4a", "#0E0E0E"]
mapper = LinearColorMapper(palette=stratify_palette, low=-1, high=1)

# Figura con fondo tipo transparente
p = figure(title="Mapa de Correlaci√≥n de Variables de Riesgo de Mora",
           x_range=variables,
           y_range=list(reversed(variables)),
           x_axis_location="above",
           tools="",
           toolbar_location=None,
           width=600, height=600,
           background_fill_color=None,  # "transparente"
           border_fill_color=None)

# Rect√°ngulos
p.rect(x="var1", y="var2", width=1, height=1,
       source=source,
       line_color=None,
       fill_color=transform('correlation', mapper),
       fill_alpha=1.0)  # aseg√∫rate de que las celdas s√≠ se vean

# Etiquetas internas
label_source = ColumnDataSource(data=dict(
    x=corr_long["var1"],
    y=corr_long["var2"],
    value=corr_long["correlation"].astype(str)
))

labels = LabelSet(x='x', y='y', text='value', source=label_source,
                  text_align='center', text_baseline='middle',
                  text_font="helvetica", text_font_size="10pt",
                  text_color="#f4f4f4")
p.add_layout(labels)

# Colorbar
color_bar = ColorBar(color_mapper=mapper,
                     major_label_text_font_size="10pt",
                     ticker=BasicTicker(desired_num_ticks=10),
                     formatter=PrintfTickFormatter(format="%.2f"),
                     label_standoff=12,
                     location=(0, 0),
                     background_fill_color=None,
                     major_label_text_color="#f4f4f4")

p.add_layout(color_bar, 'right')

# Ejes con estilo premium
p.axis.axis_line_color = None
p.axis.major_tick_line_color = None
p.axis.major_label_text_font = "helvetica"
p.axis.major_label_text_font_size = "10pt"
p.axis.major_label_text_color = "#f4f4f4"
p.axis.major_label_standoff = 0
p.xaxis.major_label_orientation = np.pi / 4
p.outline_line_color = None

# Mostrar (o usar en Streamlit con st.bokeh_chart(p))
show(p)

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

features = df_merged[["cuotas_pagadas", "cuotas_mora", "cuotas_tarde", "dias_promedio"]]
scaled = StandardScaler().fit_transform(features)

kmeans = KMeans(n_clusters=3, random_state=0)
df_merged["Cluster"] = kmeans.fit_predict(scaled)

df_merged["IssueDate"] = pd.to_datetime(df_merged["IssueDate"])
tiempo_promedio = df_merged.groupby("ID Aliada")["IssueDate"].apply(lambda x: x.sort_values().diff().mean().days)

plt.figure(figsize=(10, 6))
sns.histplot(tiempo_promedio.dropna(), bins=30, kde=True)
plt.title('Distribution of Average Time Between Loans per Ally')
plt.xlabel('Average Days Between Loans')
plt.ylabel('Number of Allies')
plt.grid(axis='y')
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Convertir fechas
df["IssueDate"] = pd.to_datetime(df["IssueDate"], errors='coerce')
df = df.dropna(subset=["IssueDate"])

# KMeans clustering
features = df[["cuotas_pagadas", "cuotas_mora", "cuotas_tarde", "dias_promedio"]]
scaled = StandardScaler().fit_transform(features)
kmeans = KMeans(n_clusters=3, random_state=0)
df["Cluster"] = kmeans.fit_predict(scaled)

# Tiempo promedio entre pr√©stamos por Aliada
tiempo_promedio = df.groupby("ID Aliada")["IssueDate"].apply(lambda x: x.sort_values().diff().mean().days)
tiempo_promedio = tiempo_promedio.dropna()

# Histograma
hist, edges = np.histogram(tiempo_promedio, bins=30)

# KDE
kde = gaussian_kde(tiempo_promedio)
x_kde = np.linspace(tiempo_promedio.min(), tiempo_promedio.max(), 200)
y_kde = kde(x_kde) * len(tiempo_promedio) * (edges[1] - edges[0])

# Fuentes
source_hist = ColumnDataSource(data=dict(top=hist, left=edges[:-1], right=edges[1:]))
source_kde = ColumnDataSource(data=dict(x=x_kde, y=y_kde))

# Crear figura con fondo tipo transparente
p = figure(title="Distribuci√≥n del Tiempo Promedio entre Pr√©stamos por Aliada",
           x_axis_label="D√≠as Promedio entre Pr√©stamos",
           y_axis_label="N√∫mero de Aliadas",
           width=800, height=400,
           background_fill_color=None,
           border_fill_color=None)

# Histograma en dorado Stratify
p.quad(top='top', bottom=0, left='left', right='right', source=source_hist,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# KDE rojo elegante
p.line('x', 'y', source=source_kde, line_color="#e63946", line_width=3)

# Hover refinado
hover = HoverTool(tooltips=[
    ("Intervalo", "@left{0} - @right{0} d√≠as"),
    ("Cantidad", "@top")
])
p.add_tools(hover)

# Est√©tica de texto y ejes
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"

p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"

p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Grid sutil
p.grid.grid_line_color = "#cccccc"
p.grid.grid_line_alpha = 0.3
p.outline_line_color = None

# Mostrar
show(p)

plt.figure(figsize=(10, 6))
sns.boxplot(x=tiempo_promedio.dropna())
plt.title('Boxplot of Average Time Between Loans per Ally')
plt.xlabel('Average Days Between Loans')
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")
df["IssueDate"] = pd.to_datetime(df["IssueDate"], errors='coerce')
df = df.dropna(subset=["IssueDate"])

# Calcular tiempo promedio entre pr√©stamos por aliada
tiempo_promedio = df.groupby("ID Aliada")["IssueDate"].apply(
    lambda x: x.sort_values().diff().mean().days
).dropna()

# Calcular cuartiles e IQR
q1 = tiempo_promedio.quantile(0.25)
q2 = tiempo_promedio.quantile(0.50)
q3 = tiempo_promedio.quantile(0.75)
iqr = q3 - q1
upper = q3 + 1.5 * iqr
lower = q1 - 1.5 * iqr

# Identificar outliers
outliers = tiempo_promedio[(tiempo_promedio > upper) | (tiempo_promedio < lower)]

# Crear figura con fondo tipo PNG (transparente)
p = figure(title="Boxplot del Tiempo Promedio entre Pr√©stamos por Aliada",
           x_axis_label="D√≠as Promedio entre Pr√©stamos",
           y_range=["Average Loan Interval"],
           width=800, height=300,
           tools="pan,box_zoom,reset,save",
           background_fill_color=None,
           border_fill_color=None)

# Caja en dorado Stratify
p.hbar(y=["Average Loan Interval"], height=0.4, left=q1, right=q3,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Mediana en rojo elegante
p.segment(x0=q2, y0=0.8, x1=q2, y1=1.2, line_color="#e63946", line_width=2)

# Bigotes (whiskers) en gris claro
p.segment(x0=lower, y0=1, x1=q1, y1=1, line_color="#aaaaaa")
p.segment(x0=q3, y0=1, x1=upper, y1=1, line_color="#aaaaaa")

# Outliers en negro, m√°s visibles
p.scatter(x=outliers, y=["Average Loan Interval"] * len(outliers), size=8,
          marker="circle", fill_alpha=0.9, fill_color="#000000", line_color=None)

# Est√©tica premium
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Grid sobrio
p.grid.grid_line_color = "#cccccc"
p.grid.grid_line_alpha = 0.3
p.outline_line_color = None

# Mostrar (o usar con st.bokeh_chart(p))
show(p)

df_merged["ConversionLlamadas"] = df_merged["prestamos_outstanding"] / df_merged["Total_Llamadas"]
conversion_media = df_merged.groupby("ID Aliada")["ConversionLlamadas"].mean()

plt.figure(figsize=(10, 6))
sns.histplot(conversion_media.dropna(), bins=30, kde=True)
plt.title('Distribution of Call-to-Conversion Rate per Ally')
plt.xlabel('Conversion Rate (Loans Outstanding / Total Calls)')
plt.ylabel('Number of Allies')
plt.grid(axis='y')
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Calcular tasa de conversi√≥n
df["ConversionLlamadas"] = df["prestamos_outstanding"] / df["Total_Llamadas"].replace(0, np.nan)
conversion_media = df.groupby("ID Aliada")["ConversionLlamadas"].mean().dropna()
conversion_media = conversion_media[np.isfinite(conversion_media)]

# Verificar que haya datos
if not conversion_media.empty:
    # Histograma
    hist, edges = np.histogram(conversion_media, bins=30)

    # KDE
    kde = gaussian_kde(conversion_media)
    x_kde = np.linspace(conversion_media.min(), conversion_media.max(), 200)
    y_kde = kde(x_kde) * len(conversion_media) * (edges[1] - edges[0])

    # Fuentes para Bokeh
    source_hist = ColumnDataSource(data=dict(top=hist, left=edges[:-1], right=edges[1:]))
    source_kde = ColumnDataSource(data=dict(x=x_kde, y=y_kde))

    # Figura con estilo premium
    p = figure(title="Distribuci√≥n de la Tasa de Conversi√≥n de Llamadas por Aliada",
               x_axis_label="Tasa de Conversi√≥n (Pr√©stamos / Llamadas Totales)",
               y_axis_label="N√∫mero de Aliadas",
               width=800, height=400,
               background_fill_color=None,
               border_fill_color=None,
               tools="pan,box_zoom,reset,save")

    # Histograma en dorado Stratify
    p.quad(top='top', bottom=0, left='left', right='right', source=source_hist,
           fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

    # L√≠nea KDE en rojo elegante
    p.line('x', 'y', source=source_kde, line_color="#e63946", line_width=3)

    # Hover refinado
    hover = HoverTool(tooltips=[
        ("Rango", "@left{0.00} - @right{0.00}"),
        ("Cantidad", "@top")
    ])
    p.add_tools(hover)

    # Est√©tica de texto
    p.title.text_font = "helvetica"
    p.title.text_font_size = "14pt"
    p.title.text_color = "#1c1c1c"
    p.xaxis.axis_label_text_font = "helvetica"
    p.xaxis.axis_label_text_color = "#1c1c1c"
    p.xaxis.major_label_text_font = "helvetica"
    p.xaxis.major_label_text_color = "#333333"
    p.yaxis.axis_label_text_font = "helvetica"
    p.yaxis.axis_label_text_color = "#1c1c1c"
    p.yaxis.major_label_text_font = "helvetica"
    p.yaxis.major_label_text_color = "#333333"

    # Grid limpio
    p.grid.grid_line_color = "#cccccc"
    p.grid.grid_line_alpha = 0.3
    p.outline_line_color = None

    # Mostrar
    show(p)
else:
    print("conversion_media est√° vac√≠a despu√©s del filtrado.")

# prompt: make it a violin plot

plt.figure(figsize=(10, 6))
sns.violinplot(x=basic_demand['basic_demand'])
plt.title('Violin Plot of Basic Demand (Total Loans per Ally)')
plt.xlabel('Number of Loans')
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Calcular demanda b√°sica
basic_demand = df.groupby("ID Aliada")["LoanID"].count().reset_index()
basic_demand.columns = ["ID Aliada", "basic_demand"]

# KDE sobre datos
data = basic_demand["basic_demand"].dropna()
kde = gaussian_kde(data)
x = np.linspace(data.min(), data.max(), 200)
y = kde(x)
y_scaled = y / y.max() * 0.4

# Datos para forma del viol√≠n
x_violin = list(x) + list(x[::-1])
y_violin = list(y_scaled) + list(-y_scaled[::-1])
source_violin = ColumnDataSource(data=dict(x=x_violin, y=y_violin))

# Crear figura
p = figure(title="Distribuci√≥n tipo Violin de la Demanda B√°sica por Aliada",
           width=800, height=400, x_axis_label="N√∫mero de Pr√©stamos",
           y_axis_label="", tools="pan,box_zoom,reset,save",
           background_fill_color=None,
           border_fill_color=None)

# Viol√≠n dorado Stratify
p.patch('x', 'y', source=source_violin, fill_color="#cba660",
        fill_alpha=0.9, line_color="#cba660")

# L√≠nea de mediana en rojo
median_val = data.median()
p.line([median_val, median_val], [-0.45, 0.45], color="#e63946", line_width=2, legend_label="Mediana")

# Eje Y limitado y oculto
p.y_range.start = -0.5
p.y_range.end = 0.5
p.yaxis.visible = False

# Est√©tica texto
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"

# Grid y leyenda
p.grid.grid_line_color = "#cccccc"
p.grid.grid_line_alpha = 0.3
p.outline_line_color = None
p.legend.location = "top_right"

# Mostrar
show(p)

# Agrupar cantidad de pr√©stamos por tipo de cliente y tipo de pr√©stamo
loan_distribution = df_merged.groupby(['RecipientType', 'LoanType']).size().reset_index(name='Count')

# Calculate the sum of 'Count' for each 'RecipientType'
recipient_type_counts = loan_distribution.groupby('RecipientType')['Count'].transform('sum')

# Calculate percentage by dividing 'Count' by the total count for each RecipientType
loan_distribution['Percent'] = (loan_distribution['Count'] / recipient_type_counts) * 100

# Tabla resumen de porcentajes
loan_summary_table = loan_distribution.pivot(index='RecipientType', columns='LoanType', values='Percent').fillna(0)
print("üìã Tabla resumen - Porcentaje de tipo de pr√©stamo por tipo de cliente:")
st.dataframe(loan_summary_table)

# Gr√°fico de barras
plt.figure(figsize=(8, 5))
sns.barplot(data=loan_distribution, x='RecipientType', y='Percent', hue='LoanType')
plt.title("Distribuci√≥n de Tipo de Pr√©stamo por Tipo de Cliente")
plt.ylabel("Porcentaje (%)")
plt.xlabel("Tipo de Cliente")
plt.legend(title="Tipo de Pr√©stamo")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Agrupar y calcular porcentaje
loan_distribution = df.groupby(['RecipientType', 'LoanType']).size().reset_index(name='Count')
recipient_type_counts = loan_distribution.groupby('RecipientType')['Count'].transform('sum')
loan_distribution['Percent'] = (loan_distribution['Count'] / recipient_type_counts) * 100

loan_distribution['RecipientType'] = loan_distribution['RecipientType'].astype(str)
loan_distribution['LoanType'] = loan_distribution['LoanType'].astype(str)

# Eje X como pares categ√≥ricos
x = list(zip(loan_distribution['RecipientType'], loan_distribution['LoanType']))
loan_types = sorted(loan_distribution['LoanType'].unique().tolist())

source = ColumnDataSource(data=dict(
    x=x,
    percent=loan_distribution['Percent'],
    tipo_cliente=loan_distribution['RecipientType'],
    tipo_prestamo=loan_distribution['LoanType']
))

# Paleta Stratify personalizada (ajustable seg√∫n cantidad de categor√≠as)
stratify_palette = ["#cba660", "#4a4a4a", "#e63946"]  # Dorado, gris oscuro, rojo

# Crear figura
p = figure(x_range=FactorRange(*x), height=400, width=800,
           title="Distribuci√≥n de Tipos de Pr√©stamo por Tipo de Cliente",
           y_axis_label="Porcentaje (%)", background_fill_color=None,
           border_fill_color=None, toolbar_location=None, tools="")

# Barras coloreadas con Stratify palette
p.vbar(x='x', top='percent', width=0.9, source=source,
       fill_color=factor_cmap('x', palette=stratify_palette,
                              factors=loan_types, start=1))

# HoverTool
hover = HoverTool(tooltips=[
    ("Cliente", "@tipo_cliente"),
    ("Pr√©stamo", "@tipo_prestamo"),
    ("Porcentaje", "@percent{0.2f}%")
])
p.add_tools(hover)

# Est√©tica Stratify
p.xaxis.major_label_orientation = 1
p.xaxis.axis_label = "Tipo de Cliente"
p.y_range.start = 0
p.x_range.range_padding = 0.1
p.ygrid.grid_line_dash = 'dashed'
p.ygrid.grid_line_color = "#cccccc"
p.outline_line_color = None

p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Mostrar gr√°fico
show(p)

# Agrupaci√≥n por tipo de cliente
total_loans_by_client = df_merged.groupby('RecipientType')['LoanID'].count().reset_index(name='TotalLoans')
avg_loans_by_client = df_merged.groupby('RecipientType')['LoanID'].nunique().reset_index(name='UniqueLoanCount')

# Fusionar ambas para an√°lisis combinado
summary = pd.merge(total_loans_by_client, avg_loans_by_client, on='RecipientType')
summary['AvgLoansPerClient'] = summary['TotalLoans'] / summary['UniqueLoanCount']

print("üìã Tabla resumen - Total y promedio de pr√©stamos por tipo de cliente:")
st.dataframe(summary)

# Gr√°fico total de pr√©stamos
plt.figure(figsize=(8, 4))
sns.barplot(data=summary, x='RecipientType', y='TotalLoans', palette='Set2')
plt.title("Total de Pr√©stamos por Tipo de Cliente")
plt.ylabel("N√∫mero de Pr√©stamos")
plt.xlabel("Tipo de Cliente")
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Agrupar total y √∫nicos por tipo de cliente
total_loans_by_client = df.groupby('RecipientType')['LoanID'].count().reset_index(name='TotalLoans')
avg_loans_by_client = df.groupby('RecipientType')['LoanID'].nunique().reset_index(name='UniqueLoanCount')

# Calcular promedio por cliente
summary = pd.merge(total_loans_by_client, avg_loans_by_client, on='RecipientType')
summary['AvgLoansPerClient'] = summary['TotalLoans'] / summary['UniqueLoanCount']

# Fuente para Bokeh
source = ColumnDataSource(summary)

# Crear figura
p = figure(x_range=summary['RecipientType'].astype(str).tolist(),
           title="Total de Pr√©stamos por Tipo de Cliente",
           x_axis_label="Tipo de Cliente",
           y_axis_label="N√∫mero de Pr√©stamos",
           width=800, height=400,
           toolbar_location=None,
           background_fill_color=None,
           border_fill_color=None)

# Barras doradas estilo Stratify
p.vbar(x='RecipientType', top='TotalLoans', width=0.6, source=source,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Est√©tica de texto y ejes
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Grid y rango
p.xgrid.grid_line_color = None
p.ygrid.grid_line_dash = 'dashed'
p.ygrid.grid_line_color = "#cccccc"
p.y_range.start = 0
p.outline_line_color = None

# Mostrar
show(p)

# Agrupar cantidad de pr√©stamos por tipo de cliente
total_loans_by_client_type = df_merged.groupby('RecipientType').size().reset_index(name='TotalLoans')

# Calcular el total general de pr√©stamos
grand_total_loans = total_loans_by_client_type['TotalLoans'].sum()

# Calcular el porcentaje de pr√©stamos por tipo de cliente
total_loans_by_client_type['Percentage'] = (total_loans_by_client_type['TotalLoans'] / grand_total_loans) * 100

print("üìã Tabla resumen - Proporci√≥n de pr√©stamos por tipo de cliente:")
st.dataframe(total_loans_by_client_type)


# Gr√°fico de barras para la proporci√≥n total de pr√©stamos
plt.figure(figsize=(8, 5))
sns.barplot(data=total_loans_by_client_type, x='RecipientType', y='Percentage', palette='viridis')
plt.title("Distribuci√≥n Porcentual del Total de Pr√©stamos por Tipo de Cliente")
plt.ylabel("Porcentaje del Total de Pr√©stamos (%)")
plt.xlabel("Tipo de Cliente")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Agrupar y calcular porcentaje
total_loans_by_client_type = df.groupby('RecipientType').size().reset_index(name='TotalLoans')
grand_total_loans = total_loans_by_client_type['TotalLoans'].sum()
total_loans_by_client_type['Percentage'] = (total_loans_by_client_type['TotalLoans'] / grand_total_loans) * 100

# Datos para Bokeh
recipient_types = total_loans_by_client_type['RecipientType'].astype(str).tolist()
percentages = total_loans_by_client_type['Percentage'].tolist()

# Crear figura
p = figure(x_range=recipient_types,
           title="Distribuci√≥n Porcentual del Total de Pr√©stamos por Tipo de Cliente",
           x_axis_label="Tipo de Cliente",
           y_axis_label="Porcentaje del Total de Pr√©stamos (%)",
           width=800, height=400,
           toolbar_location=None,
           background_fill_color=None,
           border_fill_color=None)

# Dibujar barras doradas
p.vbar(x=recipient_types, top=percentages, width=0.6,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Est√©tica de texto y ejes
p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# L√≠neas y bordes
p.xgrid.grid_line_color = None
p.ygrid.grid_line_dash = 'dashed'
p.ygrid.grid_line_color = "#cccccc"
p.y_range.start = 0
p.outline_line_color = None

# Mostrar
show(p)

# Agrupar por entidad (ID Aliada) y contar los pr√©stamos
loans_per_entity = df_merged.groupby("ID Aliada")["LoanID"].count().reset_index(name="LoanCount")

# Identificar y quitar outliers (> 8 pr√©stamos)
loans_per_entity_filtered_8 = loans_per_entity[loans_per_entity['LoanCount'] <= 8]

# Calcular el promedio de pr√©stamos despu√©s de quitar outliers
average_loans_per_entity_filtered_8 = loans_per_entity_filtered_8['LoanCount'].mean()

print(f"üìã Promedio de pr√©stamos por entidad (excluyendo > 8): {average_loans_per_entity_filtered_8:.2f}")

# Mostrar la tabla resumen con el conteo por entidad despu√©s de filtrar (> 8)
print("\nüìã Conteo de pr√©stamos por entidad (despu√©s de quitar > 8):")
st.dataframe(loans_per_entity_filtered_8.head()) # Muestra las primeras filas

# Visualizaci√≥n del conteo de pr√©stamos por entidad (histograma)

plt.figure(figsize=(10, 6))
sns.histplot(loans_per_entity_filtered_8['LoanCount'], bins=20, kde=True)
plt.title('Distribuci√≥n del Conteo de Pr√©stamos por Entidad (Excluyendo > 8 Pr√©stamos)')
plt.xlabel('N√∫mero de Pr√©stamos por Entidad')
plt.ylabel('N√∫mero de Entidades')
plt.grid(axis='y')
plt.show()

# Visualizaci√≥n del conteo de pr√©stamos por entidad (boxplot)

plt.figure(figsize=(10, 6))
sns.boxplot(x=loans_per_entity_filtered_8['LoanCount'])
plt.title('Boxplot del Conteo de Pr√©stamos por Entidad (Excluyendo > 8 Pr√©stamos)')
plt.xlabel('N√∫mero de Pr√©stamos por Entidad')
plt.show()

# Visualizaci√≥n adicional: Violin plot de los datos filtrados

plt.figure(figsize=(10, 6))
sns.violinplot(x=loans_per_entity_filtered_8['LoanCount'])
plt.title('Violin Plot del Conteo de Pr√©stamos por Entidad (Excluyendo > 8 Pr√©stamos)')
plt.xlabel('N√∫mero de Pr√©stamos por Entidad')
plt.show()

output_notebook()

# Cargar y procesar datos
df = pd.read_csv("df_merged.csv")
loans_per_entity = df.groupby("ID Aliada")["LoanID"].count().reset_index(name="LoanCount")
loans_per_entity_filtered_8 = loans_per_entity[loans_per_entity['LoanCount'] <= 8]
data = loans_per_entity_filtered_8['LoanCount']

# Histograma
hist, edges = np.histogram(data, bins=20)

# KDE
kde = gaussian_kde(data)
x_kde = np.linspace(data.min(), data.max(), 200)
y_kde = kde(x_kde) * len(data) * (edges[1] - edges[0])

# ColumnDataSources
source_hist = ColumnDataSource(data=dict(top=hist, left=edges[:-1], right=edges[1:]))
source_kde = ColumnDataSource(data=dict(x=x_kde, y=y_kde))

# Crear figura con fondo transparente
p1 = figure(title="Distribuci√≥n del Conteo de Pr√©stamos por Entidad (Excluyendo > 8)",
            x_axis_label="N√∫mero de Pr√©stamos por Entidad",
            y_axis_label="N√∫mero de Entidades",
            width=800, height=400,
            background_fill_color=None,
            border_fill_color=None)

# Barras doradas
p1.quad(top='top', bottom=0, left='left', right='right', source=source_hist,
        fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# L√≠nea KDE roja elegante
p1.line('x', 'y', source=source_kde, line_color="#e63946", line_width=3)

# HoverTool
p1.add_tools(HoverTool(tooltips=[
    ("Intervalo", "@left - @right"),
    ("Cantidad", "@top")
]))

# Est√©tica de texto
p1.title.text_font = "helvetica"
p1.title.text_font_size = "14pt"
p1.title.text_color = "#1c1c1c"
p1.xaxis.axis_label_text_font = "helvetica"
p1.xaxis.axis_label_text_color = "#1c1c1c"
p1.xaxis.major_label_text_font = "helvetica"
p1.xaxis.major_label_text_color = "#333333"
p1.yaxis.axis_label_text_font = "helvetica"
p1.yaxis.axis_label_text_color = "#1c1c1c"
p1.yaxis.major_label_text_font = "helvetica"
p1.yaxis.major_label_text_color = "#333333"

# L√≠neas y grid
p1.grid.grid_line_alpha = 0.3
p1.grid.grid_line_color = "#cccccc"
p1.outline_line_color = None

# Mostrar
show(p1)

output_notebook()

# Calcular estad√≠sticas
q1 = data.quantile(0.25)
q2 = data.quantile(0.5)
q3 = data.quantile(0.75)
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr
outliers = data[(data < lower) | (data > upper)]

# Crear figura con fondo transparente
p2 = figure(title="Boxplot del Conteo de Pr√©stamos por Entidad (Excluyendo > 8)",
            x_axis_label="N√∫mero de Pr√©stamos por Entidad",
            y_range=["Pr√©stamos por Entidad"],
            width=800, height=300,
            background_fill_color=None,
            border_fill_color=None)

# Caja dorada
p2.hbar(y=["Pr√©stamos por Entidad"], height=0.4, left=q1, right=q3,
        fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Mediana en rojo elegante
p2.segment(x0=q2, y0=0.8, x1=q2, y1=1.2, line_color="#e63946", line_width=2)

# Bigotes
p2.segment(x0=lower, y0=1, x1=q1, y1=1, line_color="#aaaaaa")
p2.segment(x0=q3, y0=1, x1=upper, y1=1, line_color="#aaaaaa")

# Outliers con puntos negros visibles
p2.scatter(x=outliers, y=["Pr√©stamos por Entidad"] * len(outliers), size=7,
           marker="circle", fill_color="black", line_color=None, fill_alpha=0.8)

# Estilo de texto
p2.title.text_font = "helvetica"
p2.title.text_font_size = "14pt"
p2.title.text_color = "#1c1c1c"
p2.xaxis.axis_label_text_font = "helvetica"
p2.xaxis.axis_label_text_color = "#1c1c1c"
p2.xaxis.major_label_text_font = "helvetica"
p2.xaxis.major_label_text_color = "#333333"
p2.yaxis.major_label_text_font = "helvetica"
p2.yaxis.major_label_text_color = "#333333"

# Grid sobrio
p2.grid.grid_line_color = "#cccccc"
p2.grid.grid_line_alpha = 0.3
p2.outline_line_color = None

# Mostrar
show(p2)

output_notebook()

# KDE y viol√≠n simulado
kde_v = gaussian_kde(data)
x_v = np.linspace(data.min(), data.max(), 200)
y_v = kde_v(x_v)
y_scaled = y_v / y_v.max() * 0.4

x_violin = list(x_v) + list(x_v[::-1])
y_violin = list(y_scaled) + list(-y_scaled[::-1])

source_violin = ColumnDataSource(data=dict(x=x_violin, y=y_violin))

# Crear figura con fondo transparente
p3 = figure(title="Violin Plot del Conteo de Pr√©stamos por Entidad (Excluyendo > 8)",
            width=800, height=400,
            x_axis_label="N√∫mero de Pr√©stamos por Entidad",
            y_axis_label="Densidad (simulada)",
            background_fill_color=None,
            border_fill_color=None,
            tools="")

# Viol√≠n dorado claro
p3.patch('x', 'y', source=source_violin,
         fill_color="#cba660", fill_alpha=0.9, line_color="#cba660")

# Mediana en rojo elegante
median_val = data.median()
p3.line([median_val, median_val], [-0.45, 0.45], color="#e63946", line_width=3)

# Ajustes del eje y
p3.y_range.start = -0.5
p3.y_range.end = 0.5
p3.yaxis.visible = False

# Est√©tica del texto
p3.title.text_font = "helvetica"
p3.title.text_font_size = "14pt"
p3.title.text_color = "#1c1c1c"
p3.xaxis.axis_label_text_font = "helvetica"
p3.xaxis.axis_label_text_color = "#1c1c1c"
p3.xaxis.major_label_text_font = "helvetica"
p3.xaxis.major_label_text_color = "#333333"

p3.grid.grid_line_color = "#cccccc"
p3.grid.grid_line_alpha = 0.3
p3.outline_line_color = None

# Mostrar
show(p3)

# Agrupar y calcular monto promedio por tipo de pr√©stamo y tipo de cliente
avg_amount = df_merged.groupby(['RecipientType', 'LoanType'])['LoanAmount'].mean().reset_index()
avg_amount.rename(columns={'LoanAmount': 'AvgLoanAmount'}, inplace=True)

# Tabla resumen
pivot_amount = avg_amount.pivot(index='RecipientType', columns='LoanType', values='AvgLoanAmount')
print(" Tabla resumen - Monto promedio por tipo de pr√©stamo y tipo de cliente:")
st.dataframe(pivot_amount)


# Gr√°fico
plt.figure(figsize=(8, 5))
sns.barplot(data=avg_amount, x='RecipientType', y='AvgLoanAmount', hue='LoanType')
plt.title("Monto Promedio del Pr√©stamo por Cliente y Tipo de Pr√©stamo")
plt.ylabel("Monto Promedio ($)")
plt.xlabel("Tipo de Cliente")
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Calcular monto promedio
avg_amount = df.groupby(['RecipientType', 'LoanType'])['LoanAmount'].mean().reset_index()
avg_amount.rename(columns={'LoanAmount': 'AvgLoanAmount'}, inplace=True)

# Asegurar strings
avg_amount['RecipientType'] = avg_amount['RecipientType'].astype(str)
avg_amount['LoanType'] = avg_amount['LoanType'].astype(str)

# Eje X como pares (RecipientType, LoanType)
recipient_types = sorted(avg_amount['RecipientType'].unique().tolist())
loan_types = sorted(avg_amount['LoanType'].unique().tolist())
x = [(rec, loan) for rec in recipient_types for loan in loan_types]

# Extraer montos
amounts = []
for rec in recipient_types:
    for loan in loan_types:
        value = avg_amount.loc[(avg_amount['RecipientType'] == rec) & (avg_amount['LoanType'] == loan), 'AvgLoanAmount']
        amounts.append(value.values[0] if not value.empty else 0)

# Fuente de datos
source = ColumnDataSource(data=dict(x=x, amounts=amounts))

# Crear figura con fondo transparente
p = figure(x_range=FactorRange(*x), height=400, width=800,
           title="Monto Promedio del Pr√©stamo por Cliente y Tipo de Pr√©stamo",
           y_axis_label="Monto Promedio ($)",
           x_axis_label="Tipo de Cliente",
           toolbar_location=None,
           background_fill_color=None,
           border_fill_color=None)

# Barras con paleta sobria (dorada y tonos c√°lidos)
stratify_palette = ["#cba660", "#000000", "#4a4a4a"]
p.vbar(x='x', top='amounts', width=0.8, source=source,
       fill_color=factor_cmap('x', palette=stratify_palette,
                              factors=loan_types, start=1))

# Est√©tica del eje y texto
p.x_range.range_padding = 0.1
p.xaxis.major_label_orientation = 1
p.y_range.start = 0
p.ygrid.grid_line_dash = 'dashed'
p.xgrid.grid_line_color = None

p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Mostrar gr√°fico
show(p)

# Conteo por canal
channel_counts = df_merged['canal_ajustado'].value_counts().reset_index()
channel_counts.columns = ['Canal', 'Cantidad']

# Bar chart - participaci√≥n por canal
plt.figure(figsize=(8, 5))
sns.barplot(x='Canal', y='Cantidad', data=channel_counts, palette='viridis')
plt.title("Participaci√≥n de Pr√©stamos por Canal")
plt.xlabel("Canal")
plt.ylabel("N√∫mero de Pr√©stamos")
plt.xticks(rotation=45, ha='right') # Rotate labels for better readability if needed
plt.tight_layout()
plt.show()

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Conteo por canal
channel_counts = df['canal_ajustado'].value_counts().reset_index()
channel_counts.columns = ['Canal', 'Cantidad']

# Convertir a strings
canales = channel_counts['Canal'].astype(str).tolist()
cantidades = channel_counts['Cantidad'].tolist()

# Crear figura con fondo transparente
p = figure(x_range=canales,
           title="Participaci√≥n de Pr√©stamos por Canal",
           x_axis_label="Canal",
           y_axis_label="N√∫mero de Pr√©stamos",
           width=800, height=400,
           toolbar_location=None,
           background_fill_color=None,
           border_fill_color=None)

# Barras doradas Stratify
p.vbar(x=canales, top=cantidades, width=0.6,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Est√©tica de ejes y texto
p.xaxis.major_label_orientation = 0.8
p.y_range.start = 0
p.ygrid.grid_line_dash = 'dashed'
p.ygrid.grid_line_color = "#cccccc"
p.xgrid.grid_line_color = None
p.outline_line_color = None

p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.axis_label_text_font = "helvetica"
p.yaxis.axis_label_text_color = "#1c1c1c"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Mostrar
show(p)

# Conteo por regi√≥n
region_counts = df_merged['customer_region'].value_counts().reset_index()
region_counts.columns = ['Regi√≥n', 'Cantidad']

# Pie chart - participaci√≥n por regi√≥n
plt.figure(figsize=(6, 6))
colors = sns.color_palette("Set2")
plt.pie(region_counts['Cantidad'], labels=region_counts['Regi√≥n'], autopct='%1.1f%%', startangle=90, colors=colors)
plt.title("Participaci√≥n de Pr√©stamos por Regi√≥n")
plt.axis('equal')
plt.tight_layout()
plt.show()

from bokeh.io import output_notebook, show
from bokeh.plotting import figure
import pandas as pd

output_notebook()

# Cargar datos
df = pd.read_csv("df_merged.csv")

# Conteo por regi√≥n
region_counts = df['customer_region'].value_counts().reset_index()
region_counts.columns = ['Regi√≥n', 'Cantidad']
region_counts = region_counts.sort_values('Cantidad')

# Listas
regiones = region_counts['Regi√≥n'].astype(str).tolist()
cantidades = region_counts['Cantidad'].tolist()

# Crear figura con fondo transparente
p = figure(y_range=regiones,
           title="Participaci√≥n de Pr√©stamos por Regi√≥n",
           x_axis_label="N√∫mero de Pr√©stamos",
           width=800, height=400,
           toolbar_location=None,
           background_fill_color=None,
           border_fill_color=None)

# Dibujar barras doradas Stratify
p.hbar(y=regiones, right=cantidades, height=0.6,
       fill_color="#cba660", line_color="#cba660", fill_alpha=0.9)

# Est√©tica de texto y ejes
p.x_range.start = 0
p.ygrid.grid_line_color = None
p.xgrid.grid_line_dash = 'dashed'
p.xgrid.grid_line_color = "#cccccc"
p.outline_line_color = None

p.title.text_font = "helvetica"
p.title.text_font_size = "14pt"
p.title.text_color = "#1c1c1c"
p.xaxis.axis_label_text_font = "helvetica"
p.xaxis.axis_label_text_color = "#1c1c1c"
p.xaxis.major_label_text_font = "helvetica"
p.xaxis.major_label_text_color = "#333333"
p.yaxis.major_label_text_font = "helvetica"
p.yaxis.major_label_text_color = "#333333"

# Mostrar gr√°fico
show(p)